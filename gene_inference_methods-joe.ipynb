{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gene_inference' from 'gene_inference/__init__.pyc'>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from itertools import repeat\n",
    "import data, data.gene_datasets\n",
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import gene_inference\n",
    "from gene_inference.models import lr, mlp, decision_tree\n",
    "from gene_inference.infer_genes import infer_gene, infer_all_genes, sample_neighbors\n",
    "import models, models.graphLayer\n",
    "from models.models import CGN\n",
    "import data, data.gene_datasets\n",
    "from data.graph import Graph\n",
    "from data.utils import split_dataset\n",
    "import optimization\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from analysis.metrics import record_metrics_for_epoch\n",
    "import analysis\n",
    "reload(analysis.metrics)\n",
    "reload(gene_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "#tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "tcgatissue = data.gene_datasets.TCGATissue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.seed = 0\n",
    "opt.nb_class = None\n",
    "opt.nb_examples = None\n",
    "opt.nb_nodes = None\n",
    "opt.graph = \"pathway\"\n",
    "opt.dataset = tcgatissue\n",
    "opt.add_self = True\n",
    "opt.norm_adj = True\n",
    "opt.add_connectivity = True\n",
    "opt.num_layer = 1\n",
    "opt.cuda = True\n",
    "opt.pool_graph = \"ignore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "path = \"/data/lisa/data/genomics/graph/pancan-tissue-graph.hdf5\"\n",
    "graph.load_graph(path)\n",
    "#graph.intersection_with(tcgatissue)\n",
    "g = nx.from_numpy_matrix(graph.adj)\n",
    "mapping = dict(zip(range(0, len(tcgatissue.df.columns)), tcgatissue.df.columns))\n",
    "g = nx.relabel_nodes(g, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def pytorch_loop(dataset, method, seed, train_size, test_size, penalty=False, num_epochs=100, num_channel=None, num_layer=None, add_emb=None, use_gate=False, dropout=True, cuda=False, adj=None):\n",
    "#     scores = []\n",
    "   \n",
    "#     labels = torch.LongTensor(dataset.labels)\n",
    "#     criterions = otim.get_criterion(dataset)\n",
    "#     train_ratio = float(train_size) / (2 * test_size + train_size)\n",
    "#     train_set, valid_set, test_set = split_dataset(dataset, batch_size=10, random=True, train_ratio=train_ratio, seed=seed, nb_samples=train_size + 2 * test_size, nb_per_class=None)\n",
    "#     patience = 20\n",
    "#     opt.num_layer = num_layer\n",
    "#     adj_transform, aggregate_function = models.graphLayer.get_transform(opt, adj)\n",
    "#     model = models.models.CGN(\n",
    "#             nb_nodes=len(dataset.df.columns), \n",
    "#             input_dim=1,\n",
    "#             channels=[num_channel] * num_layer,\n",
    "#             adj=adj,\n",
    "#             out_dim=2,\n",
    "#             on_cuda=cuda,\n",
    "#             add_emb=add_emb,\n",
    "#             transform_adj=adj_transform,\n",
    "#             aggregate_adj=aggregate_function,\n",
    "#             use_gate=use_gate,\n",
    "#             dropout=dropout,\n",
    "#             )\n",
    "#     if cuda:\n",
    "#         torch.cuda.manual_seed(trial)\n",
    "#         torch.cuda.manual_seed_all(trial)\n",
    "#         model.cuda()\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "#     max_valid = 0\n",
    "#     for t in range(0, num_epochs):\n",
    "#         start_timer = time.time()\n",
    "#         for no_b, mini in enumerate(train_set):\n",
    "#             inputs, labels = mini['sample'], mini['labels']\n",
    "\n",
    "#             inputs = Variable(inputs, requires_grad=False).float()\n",
    "#             if cuda:\n",
    "#                 inputs = inputs.cuda()\n",
    "#                 labels = labels.cuda()\n",
    "\n",
    "#             model.train()\n",
    "#             y_pred = model(inputs)\n",
    "\n",
    "#             # Compute and print loss\n",
    "#             crit_loss = otim.compute_loss(criterions, y_pred, labels)\n",
    "#             total_loss = crit_loss\n",
    "\n",
    "#             # Zero gradients, perform a backward pass, and update the weights.\n",
    "#             optimizer.zero_grad()\n",
    "#             crit_loss.backward()\n",
    "#             optimizer.step()\n",
    "#             model.eval()\n",
    "#         time_this_epoch = time.time() - start_timer\n",
    "        \n",
    "        \n",
    "#         acc = {}\n",
    "#         auc = {}\n",
    "#         for my_set, set_name in zip([train_set, valid_set], ['train', 'valid']):\n",
    "#             acc[set_name] = accuracy(my_set, my_model, on_cuda=cuda)\n",
    "#             auc[set_name] = auc(my_set, my_model, on_cuda=cuda)\n",
    "        \n",
    "        \n",
    "#         acc, auc = record_metrics_for_epoch(None, crit_loss, total_loss, t, time_this_epoch, train_set, valid_set, test_set, model, dataset, cuda=True)\n",
    "#         summary = [\n",
    "#             t,\n",
    "#             crit_loss.data[0],\n",
    "#             acc['train'],\n",
    "#             acc['valid'],\n",
    "#             auc['train'],\n",
    "#             auc['valid'],\n",
    "#             time_this_epoch\n",
    "#         ]\n",
    "#         summary = \"epoch {}, cross_loss: {:.03f}, acc_train: {:0.3f}, acc_valid: {:0.3f}, auc_train: {:0.3f}, auc_valid:{:0.3f}, time: {:.02f} sec\".format(*summary)\n",
    "#         print summary\n",
    "\n",
    "#         patience = patience - 1\n",
    "#         if patience == 0:\n",
    "#             break\n",
    "#         if max_valid < auc['valid']:\n",
    "#             max_valid = auc['valid']\n",
    "#         if max_valid > auc['valid'] and t > 15:\n",
    "#             scores.append(auc['test']) \n",
    "#             break\n",
    "\n",
    "#     return np.round(np.mean(scores), 2),  np.round(np.std(scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "\n",
    "class Method:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SkLearn(Method):\n",
    "    \n",
    "    def __init__(self, model, penalty=False):\n",
    "        self.model = model\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "\n",
    "        if self.model == \"LR\":\n",
    "            model = sklearn.linear_model.LogisticRegression()\n",
    "            if self.penalty:\n",
    "                model = sklearn.linear_model.LogisticRegression(penalty='l1', tol=0.0001)\n",
    "        elif self.model == \"DT\":\n",
    "            model = sklearn.tree.DecisionTreeClassifier()\n",
    "        elif self.model == \"MLP\":\n",
    "            model = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(32,3), learning_rate_init=0.001, early_stopping=False,  max_iter=1000)\n",
    "        else:\n",
    "            print \"incorrect label\"\n",
    "        \n",
    "        model = model.fit(X_train, y_train)\n",
    "        return sklearn.metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "class PyTorch(Method):    \n",
    "    \n",
    "    def __init__(self, model, num_epochs=100, num_channel=64, num_layer=3, add_emb=32, use_gate=False, dropout=True, cuda=True):\n",
    "        self.model = model\n",
    "        self.batch_size = 10\n",
    "        self.num_channel = num_channel\n",
    "        self.num_layer = num_layer\n",
    "        self.add_emb = add_emb\n",
    "        self.use_gate = use_gate\n",
    "        self.dropout = dropout\n",
    "        self.cuda = cuda\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "    \n",
    "        #split train into valid and train\n",
    "        local_X_train, local_X_valid, local_y_train, local_y_valid = sklearn.model_selection.train_test_split(X_train, y_train, stratify=y_train, train_size=0.60, random_state=seed)\n",
    "    \n",
    "    \n",
    "        local_X_train = torch.FloatTensor(np.expand_dims(local_X_train, axis=2))\n",
    "        local_X_valid = torch.FloatTensor(np.expand_dims(local_X_valid, axis=2))\n",
    "        X_test = torch.FloatTensor(np.expand_dims(X_test, axis=2))\n",
    "        \n",
    "        local_y_train = torch.FloatTensor(local_y_train)\n",
    "\n",
    "        criterion = optimization.get_criterion(dataset)\n",
    "        \n",
    "        patience = 20\n",
    "        opt.num_layer = self.num_layer\n",
    "        adj_transform, aggregate_function = models.graphLayer.get_transform(opt, adj)\n",
    "        model = models.models.CGN(\n",
    "                nb_nodes=len(dataset.df.columns), \n",
    "                input_dim=1,\n",
    "                channels=[self.num_channel] * self.num_layer,\n",
    "                adj=adj,\n",
    "                out_dim=2,\n",
    "                on_cuda=self.cuda,\n",
    "                add_emb=self.add_emb,\n",
    "                transform_adj=adj_transform,\n",
    "                aggregate_adj=aggregate_function,\n",
    "                use_gate=self.use_gate,\n",
    "                dropout=self.dropout,\n",
    "                )\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        max_valid = 0\n",
    "        for t in range(0, self.num_epochs):\n",
    "            start_timer = time.time()\n",
    "            \n",
    "            if self.cuda:\n",
    "                model.cuda()\n",
    "                model.on_cuda = True\n",
    "            \n",
    "            for base_x in range(0,local_X_train.shape[0], self.batch_size):\n",
    "                inputs, labels = local_X_train[base_x:base_x+self.batch_size], local_y_train[base_x:base_x+self.batch_size]\n",
    "\n",
    "                inputs = Variable(inputs, requires_grad=False).float()\n",
    "                if self.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                model.train()\n",
    "                y_pred = model(inputs)\n",
    "\n",
    "                # Compute and print loss\n",
    "                crit_loss = optimization.compute_loss(criterion, y_pred, labels)\n",
    "                total_loss = crit_loss\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                optimizer.zero_grad()\n",
    "                crit_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.eval()\n",
    "            time_this_epoch = time.time() - start_timer\n",
    "            \n",
    "            \n",
    "            auc = {}\n",
    "            if self.cuda:\n",
    "                model.cpu()\n",
    "                model.on_cuda = False\n",
    "\n",
    "            auc['train'] = sklearn.metrics.roc_auc_score(local_y_train.numpy(), model(Variable(local_X_train.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['valid'] = sklearn.metrics.roc_auc_score(local_y_valid, model(Variable(local_X_valid.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            auc['test'] = sklearn.metrics.roc_auc_score(y_test, model(Variable(X_test.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            \n",
    "            summary = [ t, crit_loss.data[0], auc['train'], auc['valid'], time_this_epoch ]\n",
    "            summary = \"epoch {}, cross_loss: {:.03f}, auc_train: {:0.3f}, auc_valid:{:0.3f}, time: {:.02f} sec\".format(*summary)\n",
    "            print summary\n",
    "\n",
    "            patience = patience - 1\n",
    "            if patience == 0:\n",
    "                break\n",
    "            if max_valid < auc['valid']:\n",
    "                max_valid = auc['valid']\n",
    "            if max_valid > auc['valid'] and t > 15:\n",
    "                #scores.append(auc['test']) \n",
    "                return auc['test']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "def method_comparison(results, dataset, models, gene, max_genes, trials, train_size, test_size):\n",
    "    \n",
    "    dataset = data.gene_datasets.TCGATissue()\n",
    "    dataset.df = dataset.df - dataset.df.mean()\n",
    "    \n",
    "    mean = dataset.df[gene].mean()\n",
    "    dataset.labels = [1 if x > mean else 0 for x in dataset.df[gene]]\n",
    "    full_df = dataset.df.copy(deep=True)\n",
    "    \n",
    "    print \"Max ex \", int(np.log2(max_genes))+1\n",
    "    for ex in range(4, int(np.log2(max_genes))+1):\n",
    "        \n",
    "        num_genes = 2**ex\n",
    "        num_genes = np.min([num_genes, tcgatissue.df.shape[1]])\n",
    "        print ex, num_genes\n",
    "        \n",
    "        neighbors = sample_neighbors(g, gene, num_genes, include_self=False)\n",
    "        print \"neighbors\", len(neighbors)\n",
    "        \n",
    "        if gene in neighbors:\n",
    "            neighbors.remove(gene)\n",
    "\n",
    "        dataset.df = dataset.df[list(neighbors)]\n",
    "        dataset.data = dataset.df.as_matrix()\n",
    "        \n",
    "        neighborhood = np.asarray(nx.to_numpy_matrix(nx.Graph(g.subgraph(neighbors))))\n",
    "        \n",
    "        for model in models:\n",
    "            for seed in range(trials):\n",
    "            \n",
    "                #have we already done it?\n",
    "                already_done = results[\"df\"][(results[\"df\"].gene_name == gene) & \n",
    "                                             (results[\"df\"].model == model['key']) &\n",
    "                                             (results[\"df\"].num_genes == num_genes) &\n",
    "                                             (results[\"df\"].seed == seed) &\n",
    "                                             (results[\"df\"].train_size == train_size)].shape[0] > 0\n",
    "\n",
    "                if already_done:\n",
    "                    print \"already done:\", model['key'], num_genes, seed\n",
    "                    continue\n",
    "                print \"doing:\", model['key'], num_genes, seed\n",
    "\n",
    "                result = model['method'].loop(dataset=dataset, seed=seed, train_size=train_size, test_size=test_size, adj=neighborhood)\n",
    "\n",
    "                experiment = {\"gene_name\": gene,\n",
    "                        \"model\": model['key'],\n",
    "                        \"num_genes\": num_genes, \n",
    "                        \"seed\":seed,\n",
    "                        \"train_size\": train_size,\n",
    "                        \"auc\":result\n",
    "                        }\n",
    "\n",
    "                results[\"df\"] = results[\"df\"].append(experiment, ignore_index=True)\n",
    "        dataset.df = full_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "reload(data)\n",
    "reload(models.models)\n",
    "reload(gene_inference)\n",
    "reload(gene_inference.models)\n",
    "reload(analysis.metrics)\n",
    "\n",
    "m = [\n",
    "#    {'key': 'LR-L1', 'method': SkLearn(\"LR\", penalty=True)},\n",
    "#    {'key': 'MLP', 'method': mlp},\n",
    "#    {'key': 'DT', 'method': SkLearn(\"DT\")},\n",
    "   {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': PyTorch(\"CGN\")},\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = {\"df\": pd.DataFrame(columns=['auc','gene_name', 'model', 'num_genes', 'seed', 'train_size'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n",
      "Max ex  14\n",
      "4 16\n",
      "neighbors 16\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 16 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 16 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 16 2\n",
      "5 32\n",
      "neighbors 32\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 32 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 32 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 32 2\n",
      "6 64\n",
      "neighbors 64\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 64 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 64 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 64 2\n",
      "7 128\n",
      "neighbors 128\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 128 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 128 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 128 2\n",
      "8 256\n",
      "neighbors 256\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 256 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 256 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 256 2\n",
      "9 512\n",
      "neighbors 512\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 512 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 512 1\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 512 2\n",
      "Doing drop-out\n"
     ]
    }
   ],
   "source": [
    "method_comparison(results, tcgatissue, m, gene=\"RPL5\", max_genes=10000, trials=3, train_size=100, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(results, open(\"results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pickle.load(open(\"results.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>seed</th>\n",
       "      <th>train_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.607299</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.600154</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.668459</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588687</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.814841</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.803318</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.826276</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.781280</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.841231</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.776743</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.825987</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.776583</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.810111</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.803318</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.765276</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.809767</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.734188</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.805918</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.727042</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>8192</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.735300</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>8192</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.641020</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.563752</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.725386</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.629185</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.843544</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.801853</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.854467</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.816209</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.868694</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.810367</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.833293</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.820938</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.744887</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.793627</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.808871</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.786801</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.826972</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.834846</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.818402</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.789946</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>LR-L1</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.735612</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.750864</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.805726</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>DT</td>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.634518</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.625624</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.662706</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.712854</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.904996</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.924441</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.909585</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.911169</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.931638</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.908044</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.919007</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.932443</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.919843</td>\n",
       "      <td>RPL5</td>\n",
       "      <td>CGN_3_layer_64_channel_emb_32_dropout</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         auc gene_name                                  model num_genes seed  \\\n",
       "0   0.607299      RPL5                                     LR        16    0   \n",
       "1   0.600154      RPL5                                     DT        16    0   \n",
       "2   0.668459      RPL5                                     LR        32    0   \n",
       "3   0.588687      RPL5                                     DT        32    0   \n",
       "4   0.814841      RPL5                                     LR        64    0   \n",
       "5   0.803318      RPL5                                     DT        64    0   \n",
       "6   0.826276      RPL5                                     LR       128    0   \n",
       "7   0.781280      RPL5                                     DT       128    0   \n",
       "8   0.841231      RPL5                                     LR       256    0   \n",
       "9   0.776743      RPL5                                     DT       256    0   \n",
       "10  0.825987      RPL5                                     LR       512    0   \n",
       "11  0.776583      RPL5                                     DT       512    0   \n",
       "12  0.810111      RPL5                                     LR      1024    0   \n",
       "13  0.803318      RPL5                                     DT      1024    0   \n",
       "14  0.765276      RPL5                                     LR      2048    0   \n",
       "15  0.809767      RPL5                                     DT      2048    0   \n",
       "16  0.734188      RPL5                                     LR      4096    0   \n",
       "17  0.805918      RPL5                                     DT      4096    0   \n",
       "18  0.727042      RPL5                                     LR      8192    0   \n",
       "19  0.735300      RPL5                                     DT      8192    0   \n",
       "20  0.641020      RPL5                                     LR        16    1   \n",
       "21  0.563752      RPL5                                     DT        16    1   \n",
       "22  0.725386      RPL5                                     LR        32    1   \n",
       "23  0.629185      RPL5                                     DT        32    1   \n",
       "24  0.843544      RPL5                                     LR        64    1   \n",
       "25  0.801853      RPL5                                     DT        64    1   \n",
       "26  0.854467      RPL5                                     LR       128    1   \n",
       "27  0.816209      RPL5                                     DT       128    1   \n",
       "28  0.868694      RPL5                                     LR       256    1   \n",
       "29  0.810367      RPL5                                     DT       256    1   \n",
       "..       ...       ...                                    ...       ...  ...   \n",
       "67  0.833293      RPL5                                  LR-L1       256    3   \n",
       "68  0.820938      RPL5                                  LR-L1       256    4   \n",
       "69  0.744887      RPL5                                     DT       256    2   \n",
       "70  0.793627      RPL5                                     DT       256    3   \n",
       "71  0.808871      RPL5                                     DT       256    4   \n",
       "72  0.786801      RPL5                                  LR-L1       512    0   \n",
       "73  0.826972      RPL5                                  LR-L1       512    1   \n",
       "74  0.834846      RPL5                                  LR-L1       512    2   \n",
       "75  0.818402      RPL5                                  LR-L1       512    3   \n",
       "76  0.789946      RPL5                                  LR-L1       512    4   \n",
       "77  0.735612      RPL5                                     DT       512    2   \n",
       "78  0.750864      RPL5                                     DT       512    3   \n",
       "79  0.805726      RPL5                                     DT       512    4   \n",
       "80       NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout        16    0   \n",
       "81  0.634518      RPL5  CGN_3_layer_64_channel_emb_32_dropout        16    1   \n",
       "82  0.625624      RPL5  CGN_3_layer_64_channel_emb_32_dropout        16    2   \n",
       "83  0.662706      RPL5  CGN_3_layer_64_channel_emb_32_dropout        32    0   \n",
       "84       NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout        32    1   \n",
       "85  0.712854      RPL5  CGN_3_layer_64_channel_emb_32_dropout        32    2   \n",
       "86       NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout        64    0   \n",
       "87  0.904996      RPL5  CGN_3_layer_64_channel_emb_32_dropout        64    1   \n",
       "88  0.924441      RPL5  CGN_3_layer_64_channel_emb_32_dropout        64    2   \n",
       "89  0.909585      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    0   \n",
       "90  0.911169      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    1   \n",
       "91  0.931638      RPL5  CGN_3_layer_64_channel_emb_32_dropout       128    2   \n",
       "92  0.908044      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    0   \n",
       "93  0.919007      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    1   \n",
       "94  0.932443      RPL5  CGN_3_layer_64_channel_emb_32_dropout       256    2   \n",
       "95       NaN      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    0   \n",
       "96  0.919843      RPL5  CGN_3_layer_64_channel_emb_32_dropout       512    1   \n",
       "\n",
       "   train_size  \n",
       "0         100  \n",
       "1         100  \n",
       "2         100  \n",
       "3         100  \n",
       "4         100  \n",
       "5         100  \n",
       "6         100  \n",
       "7         100  \n",
       "8         100  \n",
       "9         100  \n",
       "10        100  \n",
       "11        100  \n",
       "12        100  \n",
       "13        100  \n",
       "14        100  \n",
       "15        100  \n",
       "16        100  \n",
       "17        100  \n",
       "18        100  \n",
       "19        100  \n",
       "20        100  \n",
       "21        100  \n",
       "22        100  \n",
       "23        100  \n",
       "24        100  \n",
       "25        100  \n",
       "26        100  \n",
       "27        100  \n",
       "28        100  \n",
       "29        100  \n",
       "..        ...  \n",
       "67        100  \n",
       "68        100  \n",
       "69        100  \n",
       "70        100  \n",
       "71        100  \n",
       "72        100  \n",
       "73        100  \n",
       "74        100  \n",
       "75        100  \n",
       "76        100  \n",
       "77        100  \n",
       "78        100  \n",
       "79        100  \n",
       "80        100  \n",
       "81        100  \n",
       "82        100  \n",
       "83        100  \n",
       "84        100  \n",
       "85        100  \n",
       "86        100  \n",
       "87        100  \n",
       "88        100  \n",
       "89        100  \n",
       "90        100  \n",
       "91        100  \n",
       "92        100  \n",
       "93        100  \n",
       "94        100  \n",
       "95        100  \n",
       "96        100  \n",
       "\n",
       "[97 rows x 6 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>num_genes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"32\" valign=\"top\">RPL5</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">CGN_3_layer_64_channel_emb_32_dropout</th>\n",
       "      <th>16</th>\n",
       "      <td>0.630071</td>\n",
       "      <td>0.006289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.687780</td>\n",
       "      <td>0.035461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.914718</td>\n",
       "      <td>0.013750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.917464</td>\n",
       "      <td>0.012301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.919831</td>\n",
       "      <td>0.012220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.919843</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">DT</th>\n",
       "      <th>16</th>\n",
       "      <td>0.579611</td>\n",
       "      <td>0.014951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.608420</td>\n",
       "      <td>0.025379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.805997</td>\n",
       "      <td>0.005819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.801461</td>\n",
       "      <td>0.018177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.786899</td>\n",
       "      <td>0.027143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.768571</td>\n",
       "      <td>0.026808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.803318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.809767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.805918</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.735300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">LR</th>\n",
       "      <th>16</th>\n",
       "      <td>0.624160</td>\n",
       "      <td>0.023844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.696922</td>\n",
       "      <td>0.040253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.829192</td>\n",
       "      <td>0.020296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.840371</td>\n",
       "      <td>0.019934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.854963</td>\n",
       "      <td>0.019419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.844420</td>\n",
       "      <td>0.026068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.810111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>0.765276</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0.734188</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>0.727042</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">LR-L1</th>\n",
       "      <th>16</th>\n",
       "      <td>0.615286</td>\n",
       "      <td>0.024714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.702527</td>\n",
       "      <td>0.020841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.844253</td>\n",
       "      <td>0.013111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.837101</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.834441</td>\n",
       "      <td>0.010177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.811393</td>\n",
       "      <td>0.021832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               mean       std\n",
       "gene_name model                                 num_genes                    \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 16         0.630071  0.006289\n",
       "                                                32         0.687780  0.035461\n",
       "                                                64         0.914718  0.013750\n",
       "                                                128        0.917464  0.012301\n",
       "                                                256        0.919831  0.012220\n",
       "                                                512        0.919843       NaN\n",
       "          DT                                    16         0.579611  0.014951\n",
       "                                                32         0.608420  0.025379\n",
       "                                                64         0.805997  0.005819\n",
       "                                                128        0.801461  0.018177\n",
       "                                                256        0.786899  0.027143\n",
       "                                                512        0.768571  0.026808\n",
       "                                                1024       0.803318       NaN\n",
       "                                                2048       0.809767       NaN\n",
       "                                                4096       0.805918       NaN\n",
       "                                                8192       0.735300       NaN\n",
       "          LR                                    16         0.624160  0.023844\n",
       "                                                32         0.696922  0.040253\n",
       "                                                64         0.829192  0.020296\n",
       "                                                128        0.840371  0.019934\n",
       "                                                256        0.854963  0.019419\n",
       "                                                512        0.844420  0.026068\n",
       "                                                1024       0.810111       NaN\n",
       "                                                2048       0.765276       NaN\n",
       "                                                4096       0.734188       NaN\n",
       "                                                8192       0.727042       NaN\n",
       "          LR-L1                                 16         0.615286  0.024714\n",
       "                                                32         0.702527  0.020841\n",
       "                                                64         0.844253  0.013111\n",
       "                                                128        0.837101  0.005798\n",
       "                                                256        0.834441  0.010177\n",
       "                                                512        0.811393  0.021832"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f78c3e0ee50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAExCAYAAAD8yeIGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78c73715d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "titles = []\n",
    "labels = [\"LR\",\"DT\"]\n",
    "for i,model in enumerate([results_df_lr, results_df_dt]):\n",
    "    temp_results = model\n",
    "    lines.append(plt.errorbar(temp_results.index, temp_results['auc'], xerr=0, yerr=temp_results['std'])[0])\n",
    "    titles.append(labels[i])\n",
    "    plt.xticks(list(temp_results.index), temp_results['num_genes'], rotation=70)\n",
    "width = 0.2\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.legend(lines, titles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#results_df = pd.DataFrame(columns=['model', 'num_genes', 'gene_name', 'auc', 'std'])\n",
    "#results_df = results_df.append(data=pd.DataFrame(pd.DataFrame(data={'model':\"LR\", 'num_genes': 10.0, 'gene_name': \"RPL5\", 'auc':0.57, 'std': 0.01}, index=[0]))\n",
    "len([\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"])\n",
    "len([10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0])\n",
    "len([0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92])\n",
    "len([0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00])\n",
    "results_df = pd.DataFrame(data={'model':[\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"],\n",
    "                   'num_genes': [10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0],\n",
    "#                  'gene_name': [\"RPL5\", \"RPL5\"],\n",
    "                   'auc': [0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92],\n",
    "                   'std': [0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00]}, index=range(0, 28))\n",
    "plt.figure()\n",
    "titles = []\n",
    "for model in [\n",
    "    {'key': 'LR', 'method': lr},\n",
    "    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'Decision Tree', 'method': decision_tree},\n",
    "    {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': cgn_loop, 'num_channel': 64, 'num_layer': 3, 'add_emb': 32, 'use_gate': False, 'dropout': True, 'cuda': True},\n",
    "    ]:\n",
    "    temp_results = results_df.loc[results_df['model'] == model['key']].reset_index(drop=True)\n",
    "    lines.append(plt.errorbar(temp_results.index, temp_results['auc'], xerr=0, yerr=temp_results['std'])[0])\n",
    "    titles.append(model['key'])\n",
    "    plt.xticks(list(temp_results.index), temp_results['num_genes'], rotation=70)\n",
    "width = 0.2\n",
    "plt.title(\"Inferring the value of RPL5 with varying numbers of genes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"# genes\")\n",
    "plt.legend(lines, titles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_genes'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a gene from a growing number of Nodes\n",
    "lr_results = pd.DataFrame([])\n",
    "mlp_results = pd.DataFrame([])\n",
    "cgn_results = pd.DataFrame([])\n",
    "gene = \"RPL5\"\n",
    "max_samples = 200\n",
    "reload(data)\n",
    "reload(models)\n",
    "tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "\n",
    "for num_samples in range(10, max_samples, 20):\n",
    "    lr_row = infer_gene(lr, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    lr_results = lr_results.append(lr_row).reset_index(drop=True)\n",
    "    lr_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    cgn_row = infer_gene(cgn, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    cgn_results = cgn_results.append(cgn_row).reset_index(drop=True)\n",
    "    cgn_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    print num_genes\n",
    "    print cgn_results\n",
    "    print lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_samples'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of samples\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of samples\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
