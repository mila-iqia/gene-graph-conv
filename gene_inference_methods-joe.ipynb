{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from itertools import repeat\n",
    "import data, data.gene_datasets\n",
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import gene_inference\n",
    "#from gene_inference.infer_genes import infer_all_genes, sample_neighbors\n",
    "import models, models.graphLayer\n",
    "from models.models import CGN\n",
    "import data, data.gene_datasets\n",
    "from data.graph import Graph\n",
    "from data.utils import split_dataset\n",
    "import optimization\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from analysis.metrics import record_metrics_for_epoch\n",
    "import analysis\n",
    "reload(analysis.metrics)\n",
    "reload(gene_inference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n"
     ]
    }
   ],
   "source": [
    "#tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "tcgatissue = data.gene_datasets.TCGATissue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.seed = 0\n",
    "opt.nb_class = None\n",
    "opt.nb_examples = None\n",
    "opt.nb_nodes = None\n",
    "opt.graph = \"pathway\"\n",
    "opt.dataset = tcgatissue\n",
    "opt.add_self = True\n",
    "opt.norm_adj = True\n",
    "opt.add_connectivity = False\n",
    "opt.num_layer = 1\n",
    "opt.cuda = True\n",
    "opt.pool_graph = \"ignore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "path = \"/data/lisa/data/genomics/graph/pancan-tissue-graph.hdf5\"\n",
    "graph.load_graph(path)\n",
    "#graph.intersection_with(tcgatissue)\n",
    "g = nx.from_numpy_matrix(graph.adj)\n",
    "mapping = dict(zip(range(0, len(tcgatissue.df.columns)), tcgatissue.df.columns))\n",
    "g = nx.relabel_nodes(g, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sample_neighbors(g, gene, num_neighbors, include_self=True):\n",
    "#     results = set([])\n",
    "#     if include_self:\n",
    "#         results = set([gene])\n",
    "#     all_nodes = set(g.nodes)\n",
    "#     first_degree = set(g.neighbors(gene))\n",
    "#     second_degree = set()\n",
    "#     for x in g.neighbors(gene):\n",
    "#         second_degree = second_degree.union(set(g.neighbors(x)))\n",
    "#     while len(results) < num_neighbors:\n",
    "#         if len(first_degree) - len(results) > 0:\n",
    "#             unique = sorted(first_degree - results)\n",
    "#             results.add(unique.pop())\n",
    "#         elif len(second_degree) - len(results) > 0:\n",
    "#             unique = sorted(second_degree - results)\n",
    "#             results.add(unique.pop())\n",
    "#         else:\n",
    "#             unique = sorted(all_nodes - results)\n",
    "#             results.add(unique.pop())\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_neighbors(g, gene, num_neighbors, include_self=True):\n",
    "    results = set([])\n",
    "    if include_self:\n",
    "        results = set([gene])\n",
    "    all_nodes = set(g.nodes)\n",
    "    first_degree = set(g.neighbors(gene))\n",
    "    second_degree = set()\n",
    "    for x in g.neighbors(gene):\n",
    "        second_degree = second_degree.union(set(g.neighbors(x)))\n",
    "    while len(results) < num_neighbors:\n",
    "        if len(first_degree) - len(results) > 0:\n",
    "            unique = first_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        elif len(second_degree) - len(results) > 0:\n",
    "            unique = second_degree - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "        else:\n",
    "            unique = all_nodes - results\n",
    "            results.add(np.random.choice(list(unique)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_neighbors(g, \"RPL5\", 5, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn, sklearn.model_selection, sklearn.metrics, sklearn.linear_model, sklearn.neural_network, sklearn.tree\n",
    "import numpy as np\n",
    "\n",
    "class Method:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class SkLearn(Method):\n",
    "    \n",
    "    def __init__(self, model, penalty=False):\n",
    "        self.model = model\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "\n",
    "        if self.model == \"LR\":\n",
    "            model = sklearn.linear_model.LogisticRegression()\n",
    "            if self.penalty:\n",
    "                model = sklearn.linear_model.LogisticRegression(penalty='l1', tol=0.0001)\n",
    "        elif self.model == \"DT\":\n",
    "            model = sklearn.tree.DecisionTreeClassifier()\n",
    "        elif self.model == \"MLP\":\n",
    "            model = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(32,3), learning_rate_init=0.001, early_stopping=False,  max_iter=1000)\n",
    "        else:\n",
    "            print \"incorrect label\"\n",
    "        \n",
    "        model = model.fit(X_train, y_train)\n",
    "        return sklearn.metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "class PyTorch(Method):    \n",
    "    \n",
    "    def __init__(self, model, num_epochs=100, num_channel=64, num_layer=3, add_emb=32, use_gate=False, dropout=True, cuda=True):\n",
    "        self.model = model\n",
    "        self.batch_size = 10\n",
    "        self.num_channel = num_channel\n",
    "        self.num_layer = num_layer\n",
    "        self.add_emb = add_emb\n",
    "        self.use_gate = use_gate\n",
    "        self.dropout = dropout\n",
    "        self.cuda = cuda\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = 10\n",
    "        \n",
    "    def loop(self, dataset, seed, train_size, test_size, adj=None):\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(dataset.df, dataset.labels, stratify=dataset.labels, train_size=train_size, test_size=test_size, random_state=seed)\n",
    "    \n",
    "        #split train into valid and train\n",
    "        local_X_train, local_X_valid, local_y_train, local_y_valid = sklearn.model_selection.train_test_split(X_train, y_train, stratify=y_train, train_size=0.60, random_state=seed)\n",
    "        local_X_train = torch.FloatTensor(np.expand_dims(local_X_train, axis=2))\n",
    "        local_X_valid = torch.FloatTensor(np.expand_dims(local_X_valid, axis=2))\n",
    "        X_test = torch.FloatTensor(np.expand_dims(X_test, axis=2))\n",
    "        \n",
    "        local_y_train = torch.FloatTensor(local_y_train)\n",
    "\n",
    "        criterion = optimization.get_criterion(dataset)\n",
    "        patience = self.patience\n",
    "        opt.num_layer = self.num_layer\n",
    "        adj_transform, aggregate_function = models.graphLayer.get_transform(opt, adj)\n",
    "        \n",
    "        if self.model == \"CGN\":\n",
    "            model = models.models.CGN(\n",
    "                    nb_nodes=len(dataset.df.columns), \n",
    "                    input_dim=1,\n",
    "                    channels=[self.num_channel] * self.num_layer,\n",
    "                    adj=adj,\n",
    "                    out_dim=2,\n",
    "                    on_cuda=self.cuda,\n",
    "                    add_emb=self.add_emb,\n",
    "                    transform_adj=adj_transform,\n",
    "                    aggregate_adj=aggregate_function,\n",
    "                    use_gate=self.use_gate,\n",
    "                    dropout=self.dropout,\n",
    "                    )\n",
    "        elif self.model == \"MLP\":\n",
    "            model = models.models.MLP(\n",
    "                    len(dataset.df.columns), \n",
    "                    channels=[self.num_channel] * self.num_layer, \n",
    "                    out_dim=2, \n",
    "                    on_cuda=self.cuda, \n",
    "                    dropout=self.dropout)\n",
    "        elif self.model == \"SLR\":\n",
    "            model = SparseLogisticRegression(\n",
    "                    nb_nodes=len(dataset.df.columns), \n",
    "                    input_dim=1, \n",
    "                    adj=adj, \n",
    "                    out_dim=2, \n",
    "                    on_cuda=self.cuda)\n",
    "        elif self.model == \"LCG\":\n",
    "            model = LCG(\n",
    "                    nb_nodes=len(dataset.df.columns), \n",
    "                    input_dim=1, \n",
    "                    channels=[self.num_channel] * self.num_layer,\n",
    "                    adj=adj, \n",
    "                    out_dim=2,\n",
    "                    on_cuda=self.cuda,\n",
    "                    add_emb=self.add_emb,\n",
    "                    transform_adj=adj_transform, \n",
    "                    aggregate_adj=aggregate_function, \n",
    "                    use_gate=self.use_gate,\n",
    "                    dropout=self.dropout,\n",
    "                    attention_head=nb_attention_head, \n",
    "                    training_mode=training_mode)\n",
    "            \n",
    "            \n",
    "            \n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "        max_valid = 0\n",
    "        max_valid_test = 0\n",
    "        for t in range(0, self.num_epochs):\n",
    "            start_timer = time.time()\n",
    "            \n",
    "            for base_x in range(0,local_X_train.shape[0], self.batch_size):\n",
    "                inputs, labels = local_X_train[base_x:base_x+self.batch_size], local_y_train[base_x:base_x+self.batch_size]\n",
    "\n",
    "                inputs = Variable(inputs, requires_grad=False).float()\n",
    "                if self.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                model.train()\n",
    "                y_pred = model(inputs)\n",
    "\n",
    "                # Compute and print loss\n",
    "                crit_loss = optimization.compute_loss(criterion, y_pred, labels)\n",
    "                total_loss = crit_loss\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                optimizer.zero_grad()\n",
    "                crit_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.eval()\n",
    "\n",
    "            auc = {}\n",
    "            res = []\n",
    "            for base_x in range(0,local_X_train.shape[0], self.batch_size):\n",
    "                inputs = Variable(local_X_train[base_x:base_x+self.batch_size], requires_grad=False).float()\n",
    "                res.append(model(inputs.cuda())[:,1].data.cpu().numpy())\n",
    "            auc['train'] = sklearn.metrics.roc_auc_score(local_y_train.numpy(), np.asarray(res).flatten())\n",
    "            \n",
    "            res = []\n",
    "            for base_x in range(0,local_X_valid.shape[0], self.batch_size):\n",
    "                inputs = Variable(local_X_valid[base_x:base_x+self.batch_size], requires_grad=False).float()\n",
    "                res.append(model(inputs.cuda())[:,1].data.cpu().numpy())\n",
    "            auc['valid'] = sklearn.metrics.roc_auc_score(local_y_valid, np.asarray(res).flatten())\n",
    "            \n",
    "            res = []\n",
    "            for base_x in range(0,X_test.shape[0], self.batch_size):\n",
    "                inputs = Variable(X_test[base_x:base_x+self.batch_size], requires_grad=False).float()\n",
    "                res.append(model(inputs.cuda())[:,1].data.cpu().numpy())\n",
    "            auc['test'] = sklearn.metrics.roc_auc_score(y_test, np.asarray(res).flatten())\n",
    "            \n",
    "            \n",
    "            time_this_epoch = time.time() - start_timer\n",
    "\n",
    "#eval on cpu\n",
    "#             auc['train'] = sklearn.metrics.roc_auc_score(local_y_train.numpy(), model(Variable(local_X_train.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "#             auc['valid'] = sklearn.metrics.roc_auc_score(local_y_valid, model(Variable(local_X_valid.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "#             auc['test'] = sklearn.metrics.roc_auc_score(y_test, model(Variable(X_test.cpu(), requires_grad=False).float())[:,1].cpu().data.numpy())\n",
    "            \n",
    "            summary = [ t, crit_loss.data[0], auc['train'], auc['valid'], time_this_epoch ]\n",
    "            summary = \"epoch {}, cross_loss: {:.03f}, auc_train: {:0.3f}, auc_valid:{:0.3f}, time: {:.02f} sec\".format(*summary)\n",
    "            print summary\n",
    "\n",
    "            patience = patience - 1\n",
    "            if patience == 0:\n",
    "                return max_valid_test\n",
    "                break\n",
    "            if (max_valid < auc['valid']) and t > 5:\n",
    "                max_valid = auc['valid']\n",
    "                max_valid_test = auc['test']\n",
    "                patience = self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#             model = models.models.MLP(\n",
    "#                     20, \n",
    "#                     channels=[32] * 1,\n",
    "#                     out_dim=2, \n",
    "#                     on_cuda=True, \n",
    "#                     dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adj_transform, aggregate_function = models.graphLayer.get_transform(opt, nx.to_numpy_matrix(g))\n",
    "# model = models.models.CGN(\n",
    "#                     nb_nodes=len(tcgatissue.df.columns), \n",
    "#                     input_dim=1,\n",
    "#                     channels=[32] * 3,\n",
    "#                     adj=nx.to_numpy_matrix(g),\n",
    "#                     out_dim=2,\n",
    "#                     on_cuda=True,\n",
    "#                     add_emb=False,\n",
    "#                 transform_adj=adj_transform,\n",
    "#                     aggregate_adj=aggregate_function,\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tcgatissue.data[:10,:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# torch.FloatTensor(tcgatissue.data[:10,:20])[0].view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res = [model(Variable(x.view(1,1,-1), requires_grad=False)).cpu().data.numpy()[0][1] for x in torch.FloatTensor(tcgatissue.data[:10,:20])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model(Variable(tcgatissue.data, requires_grad=False).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#             model = models.models.CGN(\n",
    "#                     nb_nodes=len(tcgatissue.df.columns), \n",
    "#                     input_dim=1,\n",
    "#                     channels=[32] * 3,\n",
    "#                     adj=nx.to_numpy_matrix(g),\n",
    "#                     out_dim=2,\n",
    "#                     on_cuda=True,\n",
    "#                     add_emb=False,\n",
    "#                 transform_adj=None,\n",
    "#                     aggregate_adj=None,\n",
    "#                     )\n",
    "# model(Variable(local_X_train, requires_grad=False).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def method_comparison(results, dataset, models, gene, search_num_genes, trials, search_train_size, test_size):\n",
    "    \n",
    "    dataset = data.gene_datasets.TCGATissue()\n",
    "    dataset.df = dataset.df - dataset.df.mean()\n",
    "    \n",
    "    mean = dataset.df[gene].mean()\n",
    "    dataset.labels = [1 if x > mean else 0 for x in dataset.df[gene]]\n",
    "    full_df = dataset.df.copy(deep=True)\n",
    "    \n",
    "    #print \"Max ex \", int(np.log2(max_genes))+1\n",
    "    for train_size in search_train_size:\n",
    "        for ex in search_num_genes:#[16,24,32,48,64,128,256]:#range(4, int(np.log2(max_genes))+1):\n",
    "\n",
    "            num_genes = ex#2**ex\n",
    "            num_genes = np.min([num_genes, tcgatissue.df.shape[1]])\n",
    "            print ex, num_genes\n",
    "\n",
    "            neighbors = sample_neighbors(g, gene, num_genes, include_self=False)\n",
    "            print \"neighbors\", len(neighbors)\n",
    "\n",
    "            if gene in neighbors:\n",
    "                neighbors.remove(gene)\n",
    "\n",
    "            dataset.df = dataset.df[list(neighbors)]\n",
    "            dataset.data = dataset.df.as_matrix()\n",
    "\n",
    "            neighborhood = np.asarray(nx.to_numpy_matrix(nx.Graph(g.subgraph(neighbors))))\n",
    "\n",
    "            for model in models:\n",
    "                for seed in range(trials):\n",
    "\n",
    "                    #have we already done it?\n",
    "                    already_done = results[\"df\"][(results[\"df\"].gene_name == gene) & \n",
    "                                                 (results[\"df\"].model == model['key']) &\n",
    "                                                 (results[\"df\"].num_genes == num_genes) &\n",
    "                                                 (results[\"df\"].seed == seed) &\n",
    "                                                 (results[\"df\"].train_size == train_size)].shape[0] > 0\n",
    "\n",
    "                    if already_done:\n",
    "                        print \"already done:\", model['key'], num_genes, seed\n",
    "                        continue\n",
    "                    print \"doing:\", model['key'], num_genes, seed\n",
    "\n",
    "                    result = model['method'].loop(dataset=dataset, seed=seed, train_size=train_size, test_size=test_size, adj=neighborhood)\n",
    "\n",
    "                    experiment = {\"gene_name\": gene,\n",
    "                            \"model\": model['key'],\n",
    "                            \"num_genes\": num_genes, \n",
    "                            \"seed\":seed,\n",
    "                            \"train_size\": train_size,\n",
    "                            \"auc\":result\n",
    "                            }\n",
    "\n",
    "                    results[\"df\"] = results[\"df\"].append(experiment, ignore_index=True)\n",
    "                    pickle.dump(results, open(\"results-temp.pkl\", \"wb\"))\n",
    "            dataset.df = full_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "m = [\n",
    "    {'key': 'LR-L1', 'method': SkLearn(\"LR\", penalty=True)},\n",
    "#    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'DT', 'method': SkLearn(\"DT\")},\n",
    "  {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': PyTorch(\"CGN\")},\n",
    "   {'key': 'MLP-dropout', 'method': PyTorch(\"MLP\", dropout=True)},\n",
    "#   {'key': 'MLP', 'method': PyTorch(\"MLP\", dropout=False)},\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = {\"df\": pd.DataFrame(columns=['auc','gene_name', 'model', 'num_genes', 'seed', 'train_size'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pickle.load(open(\"results-temp.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting one-hot labels to integers\n",
      "100 100\n",
      "neighbors 100\n",
      "doing: LR-L1 100 0\n",
      "doing: LR-L1 100 1\n",
      "doing: LR-L1 100 2\n",
      "doing: LR-L1 100 3\n",
      "doing: LR-L1 100 4\n",
      "doing: LR-L1 100 5\n",
      "doing: LR-L1 100 6\n",
      "doing: LR-L1 100 7\n",
      "doing: LR-L1 100 8\n",
      "doing: LR-L1 100 9\n",
      "doing: DT 100 0\n",
      "doing: DT 100 1\n",
      "doing: DT 100 2\n",
      "doing: DT 100 3\n",
      "doing: DT 100 4\n",
      "doing: DT 100 5\n",
      "doing: DT 100 6\n",
      "doing: DT 100 7\n",
      "doing: DT 100 8\n",
      "doing: DT 100 9\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 100 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 100 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 100 2\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 100 3\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 100 4\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 100 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/cohenjos/.local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.680, auc_train: 0.938, auc_valid:0.951, time: 3.00 sec\n",
      "epoch 1, cross_loss: 0.690, auc_train: 0.943, auc_valid:0.960, time: 0.40 sec\n",
      "epoch 2, cross_loss: 0.673, auc_train: 0.941, auc_valid:0.962, time: 0.40 sec\n",
      "epoch 3, cross_loss: 0.615, auc_train: 0.945, auc_valid:0.968, time: 0.40 sec\n",
      "epoch 4, cross_loss: 0.619, auc_train: 0.952, auc_valid:0.971, time: 0.40 sec\n",
      "epoch 5, cross_loss: 0.646, auc_train: 0.956, auc_valid:0.969, time: 0.40 sec\n",
      "epoch 6, cross_loss: 0.634, auc_train: 0.960, auc_valid:0.966, time: 0.40 sec\n",
      "epoch 7, cross_loss: 0.491, auc_train: 0.965, auc_valid:0.969, time: 0.40 sec\n",
      "epoch 8, cross_loss: 0.756, auc_train: 0.970, auc_valid:0.969, time: 0.40 sec\n",
      "epoch 9, cross_loss: 0.649, auc_train: 0.975, auc_valid:0.969, time: 0.40 sec\n",
      "epoch 10, cross_loss: 0.450, auc_train: 0.977, auc_valid:0.968, time: 0.40 sec\n",
      "epoch 11, cross_loss: 0.606, auc_train: 0.982, auc_valid:0.966, time: 0.40 sec\n",
      "epoch 12, cross_loss: 0.749, auc_train: 0.986, auc_valid:0.962, time: 0.40 sec\n",
      "epoch 13, cross_loss: 0.665, auc_train: 0.990, auc_valid:0.955, time: 0.40 sec\n",
      "epoch 14, cross_loss: 0.824, auc_train: 0.990, auc_valid:0.959, time: 0.40 sec\n",
      "epoch 15, cross_loss: 0.694, auc_train: 0.991, auc_valid:0.963, time: 0.40 sec\n",
      "epoch 16, cross_loss: 0.620, auc_train: 0.994, auc_valid:0.961, time: 0.40 sec\n",
      "epoch 17, cross_loss: 0.493, auc_train: 0.995, auc_valid:0.962, time: 0.40 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 100 6\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.698, auc_train: 0.880, auc_valid:0.780, time: 0.40 sec\n",
      "epoch 1, cross_loss: 0.679, auc_train: 0.910, auc_valid:0.819, time: 0.40 sec\n",
      "epoch 2, cross_loss: 0.676, auc_train: 0.922, auc_valid:0.834, time: 0.40 sec\n",
      "epoch 3, cross_loss: 0.623, auc_train: 0.927, auc_valid:0.843, time: 0.40 sec\n",
      "epoch 4, cross_loss: 0.576, auc_train: 0.925, auc_valid:0.849, time: 0.40 sec\n",
      "epoch 5, cross_loss: 0.546, auc_train: 0.924, auc_valid:0.863, time: 0.40 sec\n",
      "epoch 6, cross_loss: 0.332, auc_train: 0.926, auc_valid:0.871, time: 0.40 sec\n",
      "epoch 7, cross_loss: 0.422, auc_train: 0.933, auc_valid:0.884, time: 0.40 sec\n",
      "epoch 8, cross_loss: 0.481, auc_train: 0.941, auc_valid:0.888, time: 0.40 sec\n",
      "epoch 9, cross_loss: 0.472, auc_train: 0.945, auc_valid:0.891, time: 0.40 sec\n",
      "epoch 10, cross_loss: 0.466, auc_train: 0.952, auc_valid:0.896, time: 0.40 sec\n",
      "epoch 11, cross_loss: 0.446, auc_train: 0.958, auc_valid:0.902, time: 0.40 sec\n",
      "epoch 12, cross_loss: 0.665, auc_train: 0.962, auc_valid:0.898, time: 0.40 sec\n",
      "epoch 13, cross_loss: 0.278, auc_train: 0.969, auc_valid:0.901, time: 0.40 sec\n",
      "epoch 14, cross_loss: 0.529, auc_train: 0.974, auc_valid:0.906, time: 0.40 sec\n",
      "epoch 15, cross_loss: 0.548, auc_train: 0.977, auc_valid:0.904, time: 0.40 sec\n",
      "epoch 16, cross_loss: 0.481, auc_train: 0.978, auc_valid:0.899, time: 0.40 sec\n",
      "epoch 17, cross_loss: 0.312, auc_train: 0.979, auc_valid:0.896, time: 0.40 sec\n",
      "epoch 18, cross_loss: 0.291, auc_train: 0.981, auc_valid:0.900, time: 0.40 sec\n",
      "epoch 19, cross_loss: 0.523, auc_train: 0.982, auc_valid:0.901, time: 0.40 sec\n",
      "epoch 20, cross_loss: 0.453, auc_train: 0.984, auc_valid:0.904, time: 0.40 sec\n",
      "epoch 21, cross_loss: 0.499, auc_train: 0.987, auc_valid:0.904, time: 0.40 sec\n",
      "epoch 22, cross_loss: 0.332, auc_train: 0.988, auc_valid:0.906, time: 0.40 sec\n",
      "epoch 23, cross_loss: 0.582, auc_train: 0.991, auc_valid:0.906, time: 0.40 sec\n",
      "epoch 24, cross_loss: 0.291, auc_train: 0.992, auc_valid:0.904, time: 0.40 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 100 7\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.691, auc_train: 0.935, auc_valid:0.918, time: 0.40 sec\n",
      "epoch 1, cross_loss: 0.678, auc_train: 0.940, auc_valid:0.921, time: 0.40 sec\n",
      "epoch 2, cross_loss: 0.636, auc_train: 0.946, auc_valid:0.917, time: 0.40 sec\n",
      "epoch 3, cross_loss: 0.702, auc_train: 0.944, auc_valid:0.908, time: 0.40 sec\n",
      "epoch 4, cross_loss: 0.630, auc_train: 0.943, auc_valid:0.909, time: 0.40 sec\n",
      "epoch 5, cross_loss: 0.576, auc_train: 0.946, auc_valid:0.906, time: 0.40 sec\n",
      "epoch 6, cross_loss: 0.655, auc_train: 0.950, auc_valid:0.916, time: 0.40 sec\n",
      "epoch 7, cross_loss: 0.740, auc_train: 0.959, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 8, cross_loss: 0.587, auc_train: 0.966, auc_valid:0.929, time: 0.40 sec\n",
      "epoch 9, cross_loss: 0.492, auc_train: 0.972, auc_valid:0.934, time: 0.40 sec\n",
      "epoch 10, cross_loss: 0.406, auc_train: 0.979, auc_valid:0.940, time: 0.40 sec\n",
      "epoch 11, cross_loss: 0.477, auc_train: 0.982, auc_valid:0.942, time: 0.40 sec\n",
      "epoch 12, cross_loss: 0.711, auc_train: 0.986, auc_valid:0.943, time: 0.40 sec\n",
      "epoch 13, cross_loss: 0.306, auc_train: 0.986, auc_valid:0.943, time: 0.40 sec\n",
      "epoch 14, cross_loss: 0.585, auc_train: 0.990, auc_valid:0.946, time: 0.40 sec\n",
      "epoch 15, cross_loss: 0.301, auc_train: 0.993, auc_valid:0.953, time: 0.40 sec\n",
      "epoch 16, cross_loss: 0.274, auc_train: 0.994, auc_valid:0.953, time: 0.40 sec\n",
      "epoch 17, cross_loss: 0.219, auc_train: 0.995, auc_valid:0.953, time: 0.40 sec\n",
      "epoch 18, cross_loss: 0.181, auc_train: 0.998, auc_valid:0.955, time: 0.40 sec\n",
      "epoch 19, cross_loss: 0.229, auc_train: 0.999, auc_valid:0.956, time: 0.40 sec\n",
      "epoch 20, cross_loss: 0.398, auc_train: 0.999, auc_valid:0.957, time: 0.40 sec\n",
      "epoch 21, cross_loss: 0.222, auc_train: 0.999, auc_valid:0.955, time: 0.40 sec\n",
      "epoch 22, cross_loss: 0.482, auc_train: 0.999, auc_valid:0.954, time: 0.40 sec\n",
      "epoch 23, cross_loss: 0.241, auc_train: 1.000, auc_valid:0.952, time: 0.40 sec\n",
      "epoch 24, cross_loss: 0.189, auc_train: 1.000, auc_valid:0.953, time: 0.40 sec\n",
      "epoch 25, cross_loss: 0.104, auc_train: 1.000, auc_valid:0.955, time: 0.40 sec\n",
      "epoch 26, cross_loss: 0.398, auc_train: 1.000, auc_valid:0.953, time: 0.40 sec\n",
      "epoch 27, cross_loss: 0.494, auc_train: 1.000, auc_valid:0.954, time: 0.40 sec\n",
      "epoch 28, cross_loss: 0.477, auc_train: 1.000, auc_valid:0.951, time: 0.40 sec\n",
      "epoch 29, cross_loss: 0.204, auc_train: 1.000, auc_valid:0.951, time: 0.40 sec\n",
      "epoch 30, cross_loss: 0.145, auc_train: 1.000, auc_valid:0.951, time: 0.40 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 100 8\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.658, auc_train: 0.893, auc_valid:0.956, time: 0.40 sec\n",
      "epoch 1, cross_loss: 0.692, auc_train: 0.907, auc_valid:0.972, time: 0.40 sec\n",
      "epoch 2, cross_loss: 0.653, auc_train: 0.904, auc_valid:0.974, time: 0.40 sec\n",
      "epoch 3, cross_loss: 0.650, auc_train: 0.901, auc_valid:0.974, time: 0.40 sec\n",
      "epoch 4, cross_loss: 0.563, auc_train: 0.903, auc_valid:0.975, time: 0.40 sec\n",
      "epoch 5, cross_loss: 0.400, auc_train: 0.906, auc_valid:0.977, time: 0.40 sec\n",
      "epoch 6, cross_loss: 0.507, auc_train: 0.909, auc_valid:0.980, time: 0.40 sec\n",
      "epoch 7, cross_loss: 0.550, auc_train: 0.916, auc_valid:0.982, time: 0.40 sec\n",
      "epoch 8, cross_loss: 0.385, auc_train: 0.923, auc_valid:0.982, time: 0.40 sec\n",
      "epoch 9, cross_loss: 0.449, auc_train: 0.928, auc_valid:0.984, time: 0.40 sec\n",
      "epoch 10, cross_loss: 0.424, auc_train: 0.933, auc_valid:0.985, time: 0.40 sec\n",
      "epoch 11, cross_loss: 0.448, auc_train: 0.938, auc_valid:0.985, time: 0.40 sec\n",
      "epoch 12, cross_loss: 0.652, auc_train: 0.941, auc_valid:0.984, time: 0.40 sec\n",
      "epoch 13, cross_loss: 0.446, auc_train: 0.946, auc_valid:0.984, time: 0.40 sec\n",
      "epoch 14, cross_loss: 0.382, auc_train: 0.951, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 15, cross_loss: 0.527, auc_train: 0.956, auc_valid:0.986, time: 0.53 sec\n",
      "epoch 16, cross_loss: 0.248, auc_train: 0.958, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 17, cross_loss: 0.579, auc_train: 0.960, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 18, cross_loss: 0.308, auc_train: 0.962, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 19, cross_loss: 0.352, auc_train: 0.962, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 20, cross_loss: 0.461, auc_train: 0.967, auc_valid:0.988, time: 0.40 sec\n",
      "epoch 21, cross_loss: 0.344, auc_train: 0.969, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 22, cross_loss: 0.455, auc_train: 0.975, auc_valid:0.989, time: 0.41 sec\n",
      "epoch 23, cross_loss: 0.337, auc_train: 0.972, auc_valid:0.988, time: 0.40 sec\n",
      "epoch 24, cross_loss: 0.300, auc_train: 0.974, auc_valid:0.987, time: 0.40 sec\n",
      "epoch 25, cross_loss: 0.434, auc_train: 0.976, auc_valid:0.984, time: 0.40 sec\n",
      "epoch 26, cross_loss: 0.305, auc_train: 0.976, auc_valid:0.990, time: 0.40 sec\n",
      "epoch 27, cross_loss: 0.520, auc_train: 0.978, auc_valid:0.992, time: 0.40 sec\n",
      "epoch 28, cross_loss: 0.221, auc_train: 0.980, auc_valid:0.991, time: 0.40 sec\n",
      "epoch 29, cross_loss: 0.274, auc_train: 0.982, auc_valid:0.992, time: 0.41 sec\n",
      "epoch 30, cross_loss: 0.485, auc_train: 0.985, auc_valid:0.992, time: 0.41 sec\n",
      "epoch 31, cross_loss: 0.243, auc_train: 0.987, auc_valid:0.991, time: 0.40 sec\n",
      "epoch 32, cross_loss: 0.336, auc_train: 0.988, auc_valid:0.991, time: 0.40 sec\n",
      "epoch 33, cross_loss: 0.392, auc_train: 0.988, auc_valid:0.990, time: 0.40 sec\n",
      "epoch 34, cross_loss: 0.103, auc_train: 0.987, auc_valid:0.992, time: 0.40 sec\n",
      "epoch 35, cross_loss: 0.565, auc_train: 0.989, auc_valid:0.988, time: 0.40 sec\n",
      "epoch 36, cross_loss: 0.538, auc_train: 0.990, auc_valid:0.989, time: 0.40 sec\n",
      "epoch 37, cross_loss: 0.466, auc_train: 0.990, auc_valid:0.991, time: 0.40 sec\n",
      "epoch 38, cross_loss: 0.376, auc_train: 0.992, auc_valid:0.991, time: 0.40 sec\n",
      "epoch 39, cross_loss: 0.254, auc_train: 0.994, auc_valid:0.990, time: 0.40 sec\n",
      "epoch 40, cross_loss: 0.125, auc_train: 0.994, auc_valid:0.993, time: 0.40 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 100 9\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.766, auc_train: 0.889, auc_valid:0.852, time: 0.40 sec\n",
      "epoch 1, cross_loss: 0.720, auc_train: 0.929, auc_valid:0.881, time: 0.41 sec\n",
      "epoch 2, cross_loss: 0.682, auc_train: 0.956, auc_valid:0.902, time: 0.40 sec\n",
      "epoch 3, cross_loss: 0.652, auc_train: 0.958, auc_valid:0.907, time: 0.40 sec\n",
      "epoch 4, cross_loss: 0.578, auc_train: 0.958, auc_valid:0.911, time: 0.40 sec\n",
      "epoch 5, cross_loss: 0.437, auc_train: 0.960, auc_valid:0.915, time: 0.41 sec\n",
      "epoch 6, cross_loss: 0.255, auc_train: 0.959, auc_valid:0.919, time: 0.41 sec\n",
      "epoch 7, cross_loss: 0.124, auc_train: 0.961, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 8, cross_loss: 0.031, auc_train: 0.966, auc_valid:0.924, time: 0.41 sec\n",
      "epoch 9, cross_loss: 0.114, auc_train: 0.974, auc_valid:0.923, time: 0.40 sec\n",
      "epoch 10, cross_loss: 0.077, auc_train: 0.980, auc_valid:0.919, time: 0.40 sec\n",
      "epoch 11, cross_loss: 0.093, auc_train: 0.983, auc_valid:0.921, time: 0.40 sec\n",
      "epoch 12, cross_loss: 0.044, auc_train: 0.986, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 13, cross_loss: 0.170, auc_train: 0.989, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 14, cross_loss: 0.122, auc_train: 0.991, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 15, cross_loss: 0.114, auc_train: 0.993, auc_valid:0.922, time: 0.41 sec\n",
      "epoch 16, cross_loss: 0.125, auc_train: 0.993, auc_valid:0.922, time: 0.40 sec\n",
      "epoch 17, cross_loss: 0.039, auc_train: 0.994, auc_valid:0.923, time: 0.40 sec\n",
      "epoch 18, cross_loss: 0.044, auc_train: 0.995, auc_valid:0.922, time: 0.40 sec\n",
      "epoch 19, cross_loss: 0.013, auc_train: 0.996, auc_valid:0.924, time: 0.40 sec\n",
      "epoch 20, cross_loss: 0.063, auc_train: 0.997, auc_valid:0.928, time: 0.40 sec\n",
      "epoch 21, cross_loss: 0.058, auc_train: 0.998, auc_valid:0.924, time: 0.40 sec\n",
      "epoch 22, cross_loss: 0.071, auc_train: 0.999, auc_valid:0.927, time: 0.40 sec\n",
      "epoch 23, cross_loss: 0.034, auc_train: 0.999, auc_valid:0.929, time: 0.40 sec\n",
      "epoch 24, cross_loss: 0.034, auc_train: 0.999, auc_valid:0.927, time: 0.40 sec\n",
      "epoch 25, cross_loss: 0.033, auc_train: 0.998, auc_valid:0.927, time: 0.40 sec\n",
      "epoch 26, cross_loss: 0.020, auc_train: 0.998, auc_valid:0.931, time: 0.41 sec\n",
      "epoch 27, cross_loss: 0.091, auc_train: 0.999, auc_valid:0.932, time: 0.40 sec\n",
      "epoch 28, cross_loss: 0.022, auc_train: 0.998, auc_valid:0.932, time: 0.40 sec\n",
      "epoch 29, cross_loss: 0.053, auc_train: 0.998, auc_valid:0.926, time: 0.40 sec\n",
      "epoch 30, cross_loss: 0.022, auc_train: 0.999, auc_valid:0.921, time: 0.41 sec\n",
      "epoch 31, cross_loss: 0.046, auc_train: 1.000, auc_valid:0.921, time: 0.40 sec\n",
      "epoch 32, cross_loss: 0.034, auc_train: 1.000, auc_valid:0.920, time: 0.40 sec\n",
      "epoch 33, cross_loss: 0.025, auc_train: 1.000, auc_valid:0.920, time: 0.41 sec\n",
      "epoch 34, cross_loss: 0.026, auc_train: 1.000, auc_valid:0.920, time: 0.40 sec\n",
      "epoch 35, cross_loss: 0.023, auc_train: 1.000, auc_valid:0.923, time: 0.40 sec\n",
      "epoch 36, cross_loss: 0.031, auc_train: 1.000, auc_valid:0.924, time: 0.40 sec\n",
      "epoch 37, cross_loss: 0.022, auc_train: 1.000, auc_valid:0.925, time: 0.40 sec\n",
      "epoch 38, cross_loss: 0.053, auc_train: 1.000, auc_valid:0.923, time: 0.40 sec\n",
      "already done: MLP-dropout 100 0\n",
      "already done: MLP-dropout 100 1\n",
      "already done: MLP-dropout 100 2\n",
      "already done: MLP-dropout 100 3\n",
      "already done: MLP-dropout 100 4\n",
      "doing: MLP-dropout 100 5\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.683, auc_train: 0.731, auc_valid:0.624, time: 0.21 sec\n",
      "epoch 1, cross_loss: 0.625, auc_train: 0.899, auc_valid:0.830, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.711, auc_train: 0.939, auc_valid:0.926, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.556, auc_train: 0.954, auc_valid:0.949, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.575, auc_train: 0.957, auc_valid:0.956, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.562, auc_train: 0.961, auc_valid:0.962, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.548, auc_train: 0.972, auc_valid:0.966, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.545, auc_train: 0.979, auc_valid:0.964, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.577, auc_train: 0.985, auc_valid:0.967, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.910, auc_train: 0.989, auc_valid:0.967, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.502, auc_train: 0.995, auc_valid:0.970, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.371, auc_train: 0.996, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.549, auc_train: 0.996, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.232, auc_train: 0.999, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.353, auc_train: 1.000, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.167, auc_train: 1.000, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.286, auc_train: 1.000, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.158, auc_train: 1.000, auc_valid:0.972, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.371, auc_train: 1.000, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.152, auc_train: 1.000, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.386, auc_train: 1.000, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.055, auc_train: 1.000, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.227, auc_train: 1.000, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.294, auc_train: 1.000, auc_valid:0.970, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.091, auc_train: 1.000, auc_valid:0.969, time: 0.14 sec\n",
      "doing: MLP-dropout 100 6\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.668, auc_train: 0.767, auc_valid:0.522, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.676, auc_train: 0.891, auc_valid:0.684, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.650, auc_train: 0.921, auc_valid:0.773, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.645, auc_train: 0.932, auc_valid:0.816, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.691, auc_train: 0.937, auc_valid:0.839, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.641, auc_train: 0.946, auc_valid:0.851, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.449, auc_train: 0.951, auc_valid:0.859, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.727, auc_train: 0.962, auc_valid:0.867, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.571, auc_train: 0.973, auc_valid:0.877, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.304, auc_train: 0.979, auc_valid:0.877, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.215, auc_train: 0.987, auc_valid:0.883, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.325, auc_train: 0.990, auc_valid:0.889, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.537, auc_train: 0.993, auc_valid:0.889, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.257, auc_train: 0.996, auc_valid:0.890, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.301, auc_train: 0.997, auc_valid:0.893, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.266, auc_train: 0.998, auc_valid:0.891, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.228, auc_train: 0.999, auc_valid:0.891, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.576, auc_train: 0.999, auc_valid:0.892, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.264, auc_train: 1.000, auc_valid:0.891, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.087, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.083, auc_train: 1.000, auc_valid:0.896, time: 0.15 sec\n",
      "epoch 21, cross_loss: 0.191, auc_train: 1.000, auc_valid:0.896, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.045, auc_train: 1.000, auc_valid:0.897, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.040, auc_train: 1.000, auc_valid:0.896, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.013, auc_train: 1.000, auc_valid:0.897, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.093, auc_train: 1.000, auc_valid:0.897, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.380, auc_train: 1.000, auc_valid:0.897, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.040, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 28, cross_loss: 0.040, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 29, cross_loss: 0.252, auc_train: 1.000, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 30, cross_loss: 0.210, auc_train: 1.000, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 31, cross_loss: 0.145, auc_train: 1.000, auc_valid:0.906, time: 0.14 sec\n",
      "epoch 32, cross_loss: 0.017, auc_train: 1.000, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 33, cross_loss: 0.041, auc_train: 1.000, auc_valid:0.906, time: 0.14 sec\n",
      "epoch 34, cross_loss: 0.016, auc_train: 1.000, auc_valid:0.905, time: 0.14 sec\n",
      "epoch 35, cross_loss: 0.191, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 36, cross_loss: 0.008, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 37, cross_loss: 0.007, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 38, cross_loss: 0.023, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 39, cross_loss: 0.007, auc_train: 1.000, auc_valid:0.904, time: 0.14 sec\n",
      "epoch 40, cross_loss: 0.080, auc_train: 1.000, auc_valid:0.909, time: 0.14 sec\n",
      "epoch 41, cross_loss: 0.038, auc_train: 1.000, auc_valid:0.909, time: 0.14 sec\n",
      "epoch 42, cross_loss: 0.201, auc_train: 1.000, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 43, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.904, time: 0.14 sec\n",
      "epoch 44, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 45, cross_loss: 0.121, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 46, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 47, cross_loss: 0.012, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 48, cross_loss: 0.026, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 49, cross_loss: 0.003, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 50, cross_loss: 0.004, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "doing: MLP-dropout 100 7\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.738, auc_train: 0.772, auc_valid:0.818, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.685, auc_train: 0.858, auc_valid:0.869, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.683, auc_train: 0.911, auc_valid:0.884, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.653, auc_train: 0.940, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.566, auc_train: 0.954, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.475, auc_train: 0.963, auc_valid:0.914, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.492, auc_train: 0.971, auc_valid:0.921, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.389, auc_train: 0.976, auc_valid:0.927, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.479, auc_train: 0.981, auc_valid:0.928, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.478, auc_train: 0.986, auc_valid:0.934, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.412, auc_train: 0.990, auc_valid:0.934, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.577, auc_train: 0.994, auc_valid:0.939, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.198, auc_train: 0.997, auc_valid:0.939, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.272, auc_train: 0.998, auc_valid:0.942, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.214, auc_train: 0.999, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.358, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.206, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.192, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.122, auc_train: 1.000, auc_valid:0.948, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.030, auc_train: 1.000, auc_valid:0.948, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.095, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.110, auc_train: 1.000, auc_valid:0.948, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.079, auc_train: 1.000, auc_valid:0.949, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.041, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.016, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.137, auc_train: 1.000, auc_valid:0.944, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.090, auc_train: 1.000, auc_valid:0.945, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.014, auc_train: 1.000, auc_valid:0.945, time: 0.14 sec\n",
      "epoch 28, cross_loss: 0.007, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 29, cross_loss: 0.113, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 30, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 31, cross_loss: 0.104, auc_train: 1.000, auc_valid:0.947, time: 0.14 sec\n",
      "epoch 32, cross_loss: 0.027, auc_train: 1.000, auc_valid:0.948, time: 0.14 sec\n",
      "doing: MLP-dropout 100 8\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.730, auc_train: 0.649, auc_valid:0.732, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.733, auc_train: 0.777, auc_valid:0.857, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.704, auc_train: 0.840, auc_valid:0.924, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.707, auc_train: 0.866, auc_valid:0.942, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.631, auc_train: 0.882, auc_valid:0.945, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.573, auc_train: 0.896, auc_valid:0.959, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.491, auc_train: 0.903, auc_valid:0.967, time: 0.15 sec\n",
      "epoch 7, cross_loss: 0.433, auc_train: 0.914, auc_valid:0.975, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.331, auc_train: 0.924, auc_valid:0.980, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.424, auc_train: 0.936, auc_valid:0.983, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.269, auc_train: 0.944, auc_valid:0.984, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.331, auc_train: 0.953, auc_valid:0.985, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.309, auc_train: 0.962, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.218, auc_train: 0.970, auc_valid:0.985, time: 0.15 sec\n",
      "epoch 14, cross_loss: 0.574, auc_train: 0.978, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.265, auc_train: 0.983, auc_valid:0.985, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.485, auc_train: 0.986, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.135, auc_train: 0.990, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.082, auc_train: 0.991, auc_valid:0.984, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.220, auc_train: 0.994, auc_valid:0.983, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.380, auc_train: 0.996, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.337, auc_train: 0.996, auc_valid:0.983, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.081, auc_train: 0.997, auc_valid:0.982, time: 0.14 sec\n",
      "doing: MLP-dropout 100 9\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.725, auc_train: 0.734, auc_valid:0.672, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.689, auc_train: 0.866, auc_valid:0.779, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.676, auc_train: 0.934, auc_valid:0.831, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.600, auc_train: 0.957, auc_valid:0.857, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.477, auc_train: 0.963, auc_valid:0.873, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.291, auc_train: 0.966, auc_valid:0.885, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.196, auc_train: 0.970, auc_valid:0.891, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.114, auc_train: 0.978, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.058, auc_train: 0.985, auc_valid:0.904, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.055, auc_train: 0.992, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.071, auc_train: 0.996, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.029, auc_train: 0.998, auc_valid:0.903, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.030, auc_train: 0.999, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.037, auc_train: 0.999, auc_valid:0.906, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.014, auc_train: 0.999, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.011, auc_train: 1.000, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.014, auc_train: 1.000, auc_valid:0.915, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.009, auc_train: 1.000, auc_valid:0.917, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.003, auc_train: 1.000, auc_valid:0.916, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.060, auc_train: 1.000, auc_valid:0.914, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.916, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.004, auc_train: 1.000, auc_valid:0.914, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.913, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.912, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.914, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.011, auc_train: 1.000, auc_valid:0.919, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.917, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.003, auc_train: 1.000, auc_valid:0.917, time: 0.14 sec\n",
      "epoch 28, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.915, time: 0.14 sec\n",
      "epoch 29, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.915, time: 0.14 sec\n",
      "epoch 30, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.916, time: 0.14 sec\n",
      "epoch 31, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.917, time: 0.14 sec\n",
      "epoch 32, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.916, time: 0.14 sec\n",
      "epoch 33, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.916, time: 0.14 sec\n",
      "epoch 34, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.915, time: 0.14 sec\n",
      "epoch 35, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.914, time: 0.14 sec\n",
      "500 500\n",
      "neighbors 500\n",
      "doing: LR-L1 500 0\n",
      "doing: LR-L1 500 1\n",
      "doing: LR-L1 500 2\n",
      "doing: LR-L1 500 3\n",
      "doing: LR-L1 500 4\n",
      "doing: LR-L1 500 5\n",
      "doing: LR-L1 500 6\n",
      "doing: LR-L1 500 7\n",
      "doing: LR-L1 500 8\n",
      "doing: LR-L1 500 9\n",
      "doing: DT 500 0\n",
      "doing: DT 500 1\n",
      "doing: DT 500 2\n",
      "doing: DT 500 3\n",
      "doing: DT 500 4\n",
      "doing: DT 500 5\n",
      "doing: DT 500 6\n",
      "doing: DT 500 7\n",
      "doing: DT 500 8\n",
      "doing: DT 500 9\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 500 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 500 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 500 2\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 500 3\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 500 4\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 500 5\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.719, auc_train: 0.902, auc_valid:0.927, time: 0.53 sec\n",
      "epoch 1, cross_loss: 0.655, auc_train: 0.941, auc_valid:0.956, time: 0.53 sec\n",
      "epoch 2, cross_loss: 0.619, auc_train: 0.957, auc_valid:0.964, time: 0.51 sec\n",
      "epoch 3, cross_loss: 0.602, auc_train: 0.963, auc_valid:0.969, time: 0.51 sec\n",
      "epoch 4, cross_loss: 0.499, auc_train: 0.968, auc_valid:0.971, time: 0.51 sec\n",
      "epoch 5, cross_loss: 0.616, auc_train: 0.974, auc_valid:0.973, time: 0.51 sec\n",
      "epoch 6, cross_loss: 0.979, auc_train: 0.979, auc_valid:0.972, time: 0.51 sec\n",
      "epoch 7, cross_loss: 0.665, auc_train: 0.985, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 8, cross_loss: 0.568, auc_train: 0.993, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 9, cross_loss: 0.398, auc_train: 0.997, auc_valid:0.978, time: 0.51 sec\n",
      "epoch 10, cross_loss: 0.305, auc_train: 0.999, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 11, cross_loss: 0.214, auc_train: 1.000, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 12, cross_loss: 0.118, auc_train: 1.000, auc_valid:0.978, time: 0.51 sec\n",
      "epoch 13, cross_loss: 0.138, auc_train: 1.000, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 14, cross_loss: 0.173, auc_train: 1.000, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 15, cross_loss: 0.101, auc_train: 1.000, auc_valid:0.981, time: 0.51 sec\n",
      "epoch 16, cross_loss: 0.037, auc_train: 1.000, auc_valid:0.979, time: 0.51 sec\n",
      "epoch 17, cross_loss: 0.088, auc_train: 1.000, auc_valid:0.979, time: 0.51 sec\n",
      "epoch 18, cross_loss: 0.069, auc_train: 1.000, auc_valid:0.984, time: 0.51 sec\n",
      "epoch 19, cross_loss: 0.038, auc_train: 1.000, auc_valid:0.980, time: 0.51 sec\n",
      "epoch 20, cross_loss: 0.025, auc_train: 1.000, auc_valid:0.976, time: 0.51 sec\n",
      "epoch 21, cross_loss: 0.450, auc_train: 1.000, auc_valid:0.976, time: 0.52 sec\n",
      "epoch 22, cross_loss: 0.101, auc_train: 1.000, auc_valid:0.980, time: 0.51 sec\n",
      "epoch 23, cross_loss: 0.055, auc_train: 1.000, auc_valid:0.981, time: 0.51 sec\n",
      "epoch 24, cross_loss: 0.077, auc_train: 1.000, auc_valid:0.979, time: 0.51 sec\n",
      "epoch 25, cross_loss: 0.017, auc_train: 1.000, auc_valid:0.977, time: 0.51 sec\n",
      "epoch 26, cross_loss: 0.132, auc_train: 1.000, auc_valid:0.976, time: 0.51 sec\n",
      "epoch 27, cross_loss: 0.058, auc_train: 1.000, auc_valid:0.975, time: 0.51 sec\n",
      "epoch 28, cross_loss: 0.079, auc_train: 1.000, auc_valid:0.977, time: 0.51 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 500 6\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.707, auc_train: 0.897, auc_valid:0.788, time: 0.52 sec\n",
      "epoch 1, cross_loss: 0.740, auc_train: 0.944, auc_valid:0.827, time: 0.51 sec\n",
      "epoch 2, cross_loss: 0.667, auc_train: 0.951, auc_valid:0.837, time: 0.51 sec\n",
      "epoch 3, cross_loss: 0.663, auc_train: 0.949, auc_valid:0.851, time: 0.51 sec\n",
      "epoch 4, cross_loss: 0.545, auc_train: 0.943, auc_valid:0.862, time: 0.51 sec\n",
      "epoch 5, cross_loss: 0.549, auc_train: 0.950, auc_valid:0.877, time: 0.51 sec\n",
      "epoch 6, cross_loss: 0.495, auc_train: 0.968, auc_valid:0.900, time: 0.51 sec\n",
      "epoch 7, cross_loss: 0.642, auc_train: 0.980, auc_valid:0.902, time: 0.51 sec\n",
      "epoch 8, cross_loss: 0.477, auc_train: 0.987, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 9, cross_loss: 0.379, auc_train: 0.994, auc_valid:0.907, time: 0.51 sec\n",
      "epoch 10, cross_loss: 0.254, auc_train: 0.998, auc_valid:0.911, time: 0.51 sec\n",
      "epoch 11, cross_loss: 0.209, auc_train: 0.999, auc_valid:0.913, time: 0.51 sec\n",
      "epoch 12, cross_loss: 0.069, auc_train: 1.000, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 13, cross_loss: 0.109, auc_train: 1.000, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 14, cross_loss: 0.217, auc_train: 1.000, auc_valid:0.915, time: 0.51 sec\n",
      "epoch 15, cross_loss: 0.465, auc_train: 0.999, auc_valid:0.915, time: 0.51 sec\n",
      "epoch 16, cross_loss: 0.101, auc_train: 1.000, auc_valid:0.916, time: 0.51 sec\n",
      "epoch 17, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 18, cross_loss: 0.127, auc_train: 1.000, auc_valid:0.904, time: 0.51 sec\n",
      "epoch 19, cross_loss: 0.075, auc_train: 1.000, auc_valid:0.906, time: 0.51 sec\n",
      "epoch 20, cross_loss: 0.184, auc_train: 1.000, auc_valid:0.906, time: 0.51 sec\n",
      "epoch 21, cross_loss: 0.424, auc_train: 1.000, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 22, cross_loss: 0.132, auc_train: 1.000, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 23, cross_loss: 0.101, auc_train: 1.000, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 24, cross_loss: 0.084, auc_train: 1.000, auc_valid:0.910, time: 0.51 sec\n",
      "epoch 25, cross_loss: 0.105, auc_train: 1.000, auc_valid:0.907, time: 0.51 sec\n",
      "epoch 26, cross_loss: 0.054, auc_train: 1.000, auc_valid:0.902, time: 0.51 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 500 7\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.697, auc_train: 0.948, auc_valid:0.899, time: 0.52 sec\n",
      "epoch 1, cross_loss: 0.685, auc_train: 0.962, auc_valid:0.904, time: 0.51 sec\n",
      "epoch 2, cross_loss: 0.653, auc_train: 0.963, auc_valid:0.904, time: 0.51 sec\n",
      "epoch 3, cross_loss: 0.637, auc_train: 0.966, auc_valid:0.908, time: 0.51 sec\n",
      "epoch 4, cross_loss: 0.408, auc_train: 0.969, auc_valid:0.909, time: 0.51 sec\n",
      "epoch 5, cross_loss: 0.591, auc_train: 0.976, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 6, cross_loss: 0.687, auc_train: 0.979, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 7, cross_loss: 0.545, auc_train: 0.986, auc_valid:0.920, time: 0.51 sec\n",
      "epoch 8, cross_loss: 0.523, auc_train: 0.990, auc_valid:0.921, time: 0.51 sec\n",
      "epoch 9, cross_loss: 0.367, auc_train: 0.996, auc_valid:0.928, time: 0.51 sec\n",
      "epoch 10, cross_loss: 0.213, auc_train: 0.998, auc_valid:0.931, time: 0.51 sec\n",
      "epoch 11, cross_loss: 0.181, auc_train: 1.000, auc_valid:0.929, time: 0.51 sec\n",
      "epoch 12, cross_loss: 0.110, auc_train: 1.000, auc_valid:0.929, time: 0.51 sec\n",
      "epoch 13, cross_loss: 0.196, auc_train: 1.000, auc_valid:0.931, time: 0.51 sec\n",
      "epoch 14, cross_loss: 0.364, auc_train: 1.000, auc_valid:0.929, time: 0.51 sec\n",
      "epoch 15, cross_loss: 0.234, auc_train: 1.000, auc_valid:0.925, time: 0.51 sec\n",
      "epoch 16, cross_loss: 0.122, auc_train: 1.000, auc_valid:0.926, time: 0.51 sec\n",
      "epoch 17, cross_loss: 0.254, auc_train: 1.000, auc_valid:0.929, time: 0.51 sec\n",
      "epoch 18, cross_loss: 0.037, auc_train: 1.000, auc_valid:0.929, time: 0.51 sec\n",
      "epoch 19, cross_loss: 0.023, auc_train: 1.000, auc_valid:0.930, time: 0.51 sec\n",
      "epoch 20, cross_loss: 0.073, auc_train: 1.000, auc_valid:0.930, time: 0.51 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 500 8\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 0.675, auc_train: 0.917, auc_valid:0.913, time: 0.52 sec\n",
      "epoch 1, cross_loss: 0.660, auc_train: 0.931, auc_valid:0.923, time: 0.51 sec\n",
      "epoch 2, cross_loss: 0.629, auc_train: 0.928, auc_valid:0.933, time: 0.51 sec\n",
      "epoch 3, cross_loss: 0.527, auc_train: 0.929, auc_valid:0.942, time: 0.51 sec\n",
      "epoch 4, cross_loss: 0.369, auc_train: 0.937, auc_valid:0.956, time: 0.51 sec\n",
      "epoch 5, cross_loss: 0.466, auc_train: 0.947, auc_valid:0.964, time: 0.51 sec\n",
      "epoch 6, cross_loss: 0.878, auc_train: 0.957, auc_valid:0.967, time: 0.51 sec\n",
      "epoch 7, cross_loss: 0.356, auc_train: 0.971, auc_valid:0.969, time: 0.50 sec\n",
      "epoch 8, cross_loss: 0.399, auc_train: 0.978, auc_valid:0.975, time: 0.51 sec\n",
      "epoch 9, cross_loss: 0.425, auc_train: 0.987, auc_valid:0.976, time: 0.51 sec\n",
      "epoch 10, cross_loss: 0.067, auc_train: 0.991, auc_valid:0.973, time: 0.51 sec\n",
      "epoch 11, cross_loss: 0.469, auc_train: 0.994, auc_valid:0.980, time: 0.51 sec\n",
      "epoch 12, cross_loss: 0.630, auc_train: 0.996, auc_valid:0.983, time: 0.51 sec\n",
      "epoch 13, cross_loss: 0.208, auc_train: 0.998, auc_valid:0.984, time: 0.51 sec\n",
      "epoch 14, cross_loss: 0.302, auc_train: 0.999, auc_valid:0.982, time: 0.51 sec\n",
      "epoch 15, cross_loss: 0.069, auc_train: 1.000, auc_valid:0.984, time: 0.51 sec\n",
      "epoch 16, cross_loss: 0.087, auc_train: 1.000, auc_valid:0.984, time: 0.51 sec\n",
      "epoch 17, cross_loss: 0.094, auc_train: 1.000, auc_valid:0.986, time: 0.51 sec\n",
      "epoch 18, cross_loss: 0.053, auc_train: 1.000, auc_valid:0.987, time: 0.51 sec\n",
      "epoch 19, cross_loss: 0.359, auc_train: 1.000, auc_valid:0.989, time: 0.51 sec\n",
      "epoch 20, cross_loss: 0.425, auc_train: 1.000, auc_valid:0.990, time: 0.51 sec\n",
      "epoch 21, cross_loss: 0.123, auc_train: 1.000, auc_valid:0.992, time: 0.51 sec\n",
      "epoch 22, cross_loss: 0.093, auc_train: 1.000, auc_valid:0.991, time: 0.51 sec\n",
      "epoch 23, cross_loss: 0.045, auc_train: 1.000, auc_valid:0.987, time: 0.51 sec\n",
      "epoch 24, cross_loss: 0.426, auc_train: 1.000, auc_valid:0.984, time: 0.51 sec\n",
      "epoch 25, cross_loss: 0.098, auc_train: 1.000, auc_valid:0.981, time: 0.51 sec\n",
      "epoch 26, cross_loss: 0.009, auc_train: 1.000, auc_valid:0.982, time: 0.51 sec\n",
      "epoch 27, cross_loss: 0.225, auc_train: 1.000, auc_valid:0.986, time: 0.51 sec\n",
      "epoch 28, cross_loss: 0.116, auc_train: 1.000, auc_valid:0.989, time: 0.51 sec\n",
      "epoch 29, cross_loss: 0.053, auc_train: 1.000, auc_valid:0.991, time: 0.51 sec\n",
      "epoch 30, cross_loss: 0.004, auc_train: 1.000, auc_valid:0.990, time: 0.51 sec\n",
      "epoch 31, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.988, time: 0.51 sec\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 500 9\n",
      "Doing drop-out\n",
      "epoch 0, cross_loss: 1.417, auc_train: 0.774, auc_valid:0.679, time: 0.52 sec\n",
      "epoch 1, cross_loss: 0.627, auc_train: 0.829, auc_valid:0.637, time: 0.51 sec\n",
      "epoch 2, cross_loss: 0.731, auc_train: 0.925, auc_valid:0.753, time: 0.51 sec\n",
      "epoch 3, cross_loss: 0.663, auc_train: 0.954, auc_valid:0.788, time: 0.51 sec\n",
      "epoch 4, cross_loss: 0.616, auc_train: 0.956, auc_valid:0.843, time: 0.51 sec\n",
      "epoch 5, cross_loss: 0.438, auc_train: 0.961, auc_valid:0.868, time: 0.51 sec\n",
      "epoch 6, cross_loss: 0.229, auc_train: 0.968, auc_valid:0.874, time: 0.51 sec\n",
      "epoch 7, cross_loss: 0.235, auc_train: 0.980, auc_valid:0.885, time: 0.51 sec\n",
      "epoch 8, cross_loss: 0.116, auc_train: 0.989, auc_valid:0.894, time: 0.51 sec\n",
      "epoch 9, cross_loss: 0.089, auc_train: 0.996, auc_valid:0.897, time: 0.51 sec\n",
      "epoch 10, cross_loss: 0.076, auc_train: 0.999, auc_valid:0.893, time: 0.51 sec\n",
      "epoch 11, cross_loss: 0.062, auc_train: 1.000, auc_valid:0.905, time: 0.51 sec\n",
      "epoch 12, cross_loss: 0.084, auc_train: 1.000, auc_valid:0.902, time: 0.51 sec\n",
      "epoch 13, cross_loss: 0.198, auc_train: 1.000, auc_valid:0.907, time: 0.51 sec\n",
      "epoch 14, cross_loss: 0.063, auc_train: 1.000, auc_valid:0.916, time: 0.51 sec\n",
      "epoch 15, cross_loss: 0.133, auc_train: 1.000, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 16, cross_loss: 0.029, auc_train: 1.000, auc_valid:0.919, time: 0.51 sec\n",
      "epoch 17, cross_loss: 0.167, auc_train: 1.000, auc_valid:0.918, time: 0.51 sec\n",
      "epoch 18, cross_loss: 0.035, auc_train: 1.000, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 19, cross_loss: 0.034, auc_train: 1.000, auc_valid:0.909, time: 0.51 sec\n",
      "epoch 20, cross_loss: 0.045, auc_train: 1.000, auc_valid:0.911, time: 0.51 sec\n",
      "epoch 21, cross_loss: 0.015, auc_train: 1.000, auc_valid:0.919, time: 0.51 sec\n",
      "epoch 22, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.922, time: 0.51 sec\n",
      "epoch 23, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.921, time: 0.51 sec\n",
      "epoch 24, cross_loss: 0.098, auc_train: 1.000, auc_valid:0.919, time: 0.51 sec\n",
      "epoch 25, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.914, time: 0.51 sec\n",
      "epoch 26, cross_loss: 0.017, auc_train: 1.000, auc_valid:0.916, time: 0.51 sec\n",
      "epoch 27, cross_loss: 0.273, auc_train: 1.000, auc_valid:0.920, time: 0.51 sec\n",
      "epoch 28, cross_loss: 0.022, auc_train: 1.000, auc_valid:0.918, time: 0.51 sec\n",
      "epoch 29, cross_loss: 0.011, auc_train: 1.000, auc_valid:0.915, time: 0.51 sec\n",
      "epoch 30, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.912, time: 0.51 sec\n",
      "epoch 31, cross_loss: 0.027, auc_train: 1.000, auc_valid:0.906, time: 0.51 sec\n",
      "epoch 32, cross_loss: 0.011, auc_train: 1.000, auc_valid:0.906, time: 0.51 sec\n",
      "already done: MLP-dropout 500 0\n",
      "already done: MLP-dropout 500 1\n",
      "already done: MLP-dropout 500 2\n",
      "already done: MLP-dropout 500 3\n",
      "already done: MLP-dropout 500 4\n",
      "doing: MLP-dropout 500 5\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.717, auc_train: 0.801, auc_valid:0.813, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.646, auc_train: 0.957, auc_valid:0.912, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.611, auc_train: 0.957, auc_valid:0.955, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.533, auc_train: 0.965, auc_valid:0.959, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.508, auc_train: 0.982, auc_valid:0.971, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.527, auc_train: 0.985, auc_valid:0.976, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.361, auc_train: 0.992, auc_valid:0.978, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.388, auc_train: 0.996, auc_valid:0.977, time: 0.15 sec\n",
      "epoch 8, cross_loss: 0.268, auc_train: 0.999, auc_valid:0.980, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.075, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.145, auc_train: 1.000, auc_valid:0.977, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.047, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.091, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.021, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.057, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.291, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.161, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.084, auc_train: 1.000, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.052, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.074, auc_train: 1.000, auc_valid:0.980, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.327, auc_train: 1.000, auc_valid:0.981, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.004, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.311, auc_train: 1.000, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.014, auc_train: 1.000, auc_valid:0.980, time: 0.14 sec\n",
      "doing: MLP-dropout 500 6\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.672, auc_train: 0.833, auc_valid:0.700, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.633, auc_train: 0.924, auc_valid:0.780, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.742, auc_train: 0.958, auc_valid:0.811, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.541, auc_train: 0.962, auc_valid:0.832, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.489, auc_train: 0.969, auc_valid:0.839, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.312, auc_train: 0.976, auc_valid:0.858, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.417, auc_train: 0.989, auc_valid:0.874, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.338, auc_train: 0.996, auc_valid:0.887, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.227, auc_train: 0.998, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.191, auc_train: 0.999, auc_valid:0.903, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.368, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.173, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.045, auc_train: 1.000, auc_valid:0.891, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.175, auc_train: 1.000, auc_valid:0.886, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.019, auc_train: 1.000, auc_valid:0.892, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.889, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.022, auc_train: 1.000, auc_valid:0.896, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.031, auc_train: 1.000, auc_valid:0.900, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.020, auc_train: 1.000, auc_valid:0.900, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.043, auc_train: 1.000, auc_valid:0.902, time: 0.14 sec\n",
      "doing: MLP-dropout 500 7\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.696, auc_train: 0.839, auc_valid:0.852, time: 0.15 sec\n",
      "epoch 1, cross_loss: 0.655, auc_train: 0.937, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.576, auc_train: 0.977, auc_valid:0.896, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.591, auc_train: 0.984, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.415, auc_train: 0.989, auc_valid:0.911, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.421, auc_train: 0.995, auc_valid:0.918, time: 0.15 sec\n",
      "epoch 6, cross_loss: 0.408, auc_train: 0.998, auc_valid:0.923, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.322, auc_train: 0.999, auc_valid:0.929, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.380, auc_train: 1.000, auc_valid:0.929, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.190, auc_train: 1.000, auc_valid:0.927, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.226, auc_train: 1.000, auc_valid:0.932, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.274, auc_train: 1.000, auc_valid:0.932, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.090, auc_train: 1.000, auc_valid:0.931, time: 0.15 sec\n",
      "epoch 13, cross_loss: 0.025, auc_train: 1.000, auc_valid:0.935, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.166, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.067, auc_train: 1.000, auc_valid:0.939, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.237, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.119, auc_train: 1.000, auc_valid:0.938, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.033, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.015, auc_train: 1.000, auc_valid:0.940, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.057, auc_train: 1.000, auc_valid:0.942, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.114, auc_train: 1.000, auc_valid:0.940, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.938, time: 0.15 sec\n",
      "epoch 23, cross_loss: 0.027, auc_train: 1.000, auc_valid:0.943, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.194, auc_train: 1.000, auc_valid:0.941, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.178, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 28, cross_loss: 0.011, auc_train: 1.000, auc_valid:0.946, time: 0.14 sec\n",
      "epoch 29, cross_loss: 0.212, auc_train: 1.000, auc_valid:0.942, time: 0.14 sec\n",
      "epoch 30, cross_loss: 0.016, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 31, cross_loss: 0.004, auc_train: 1.000, auc_valid:0.939, time: 0.14 sec\n",
      "epoch 32, cross_loss: 0.102, auc_train: 1.000, auc_valid:0.938, time: 0.15 sec\n",
      "epoch 33, cross_loss: 0.047, auc_train: 1.000, auc_valid:0.938, time: 0.14 sec\n",
      "epoch 34, cross_loss: 0.006, auc_train: 1.000, auc_valid:0.938, time: 0.14 sec\n",
      "epoch 35, cross_loss: 0.009, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 36, cross_loss: 0.143, auc_train: 1.000, auc_valid:0.937, time: 0.14 sec\n",
      "epoch 37, cross_loss: 0.020, auc_train: 1.000, auc_valid:0.941, time: 0.14 sec\n",
      "epoch 38, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.939, time: 0.14 sec\n",
      "doing: MLP-dropout 500 8\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.690, auc_train: 0.839, auc_valid:0.909, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.574, auc_train: 0.896, auc_valid:0.953, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.558, auc_train: 0.924, auc_valid:0.960, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.438, auc_train: 0.946, auc_valid:0.962, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.343, auc_train: 0.961, auc_valid:0.962, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.407, auc_train: 0.969, auc_valid:0.966, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.241, auc_train: 0.982, auc_valid:0.974, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.329, auc_train: 0.988, auc_valid:0.979, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.383, auc_train: 0.991, auc_valid:0.980, time: 0.14 sec\n",
      "epoch 9, cross_loss: 0.117, auc_train: 0.996, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.251, auc_train: 0.999, auc_valid:0.984, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.358, auc_train: 1.000, auc_valid:0.984, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.316, auc_train: 0.999, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.073, auc_train: 1.000, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.035, auc_train: 1.000, auc_valid:0.987, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.173, auc_train: 1.000, auc_valid:0.989, time: 0.14 sec\n",
      "epoch 16, cross_loss: 0.180, auc_train: 1.000, auc_valid:0.988, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.041, auc_train: 0.999, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.189, auc_train: 1.000, auc_valid:0.988, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.072, auc_train: 1.000, auc_valid:0.986, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.275, auc_train: 1.000, auc_valid:0.985, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.108, auc_train: 1.000, auc_valid:0.984, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.160, auc_train: 1.000, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.196, auc_train: 1.000, auc_valid:0.982, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.047, auc_train: 1.000, auc_valid:0.984, time: 0.14 sec\n",
      "doing: MLP-dropout 500 9\n",
      "Doing Drop-out\n",
      "epoch 0, cross_loss: 0.693, auc_train: 0.813, auc_valid:0.634, time: 0.14 sec\n",
      "epoch 1, cross_loss: 0.689, auc_train: 0.928, auc_valid:0.755, time: 0.14 sec\n",
      "epoch 2, cross_loss: 0.663, auc_train: 0.951, auc_valid:0.799, time: 0.14 sec\n",
      "epoch 3, cross_loss: 0.492, auc_train: 0.963, auc_valid:0.824, time: 0.14 sec\n",
      "epoch 4, cross_loss: 0.378, auc_train: 0.976, auc_valid:0.843, time: 0.14 sec\n",
      "epoch 5, cross_loss: 0.230, auc_train: 0.987, auc_valid:0.851, time: 0.14 sec\n",
      "epoch 6, cross_loss: 0.169, auc_train: 0.994, auc_valid:0.873, time: 0.14 sec\n",
      "epoch 7, cross_loss: 0.181, auc_train: 0.997, auc_valid:0.884, time: 0.14 sec\n",
      "epoch 8, cross_loss: 0.297, auc_train: 0.999, auc_valid:0.886, time: 0.15 sec\n",
      "epoch 9, cross_loss: 0.047, auc_train: 1.000, auc_valid:0.887, time: 0.14 sec\n",
      "epoch 10, cross_loss: 0.227, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 11, cross_loss: 0.030, auc_train: 1.000, auc_valid:0.896, time: 0.14 sec\n",
      "epoch 12, cross_loss: 0.033, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "epoch 13, cross_loss: 0.043, auc_train: 1.000, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 14, cross_loss: 0.134, auc_train: 1.000, auc_valid:0.897, time: 0.14 sec\n",
      "epoch 15, cross_loss: 0.113, auc_train: 1.000, auc_valid:0.899, time: 0.15 sec\n",
      "epoch 16, cross_loss: 0.060, auc_train: 1.000, auc_valid:0.900, time: 0.14 sec\n",
      "epoch 17, cross_loss: 0.049, auc_train: 1.000, auc_valid:0.900, time: 0.14 sec\n",
      "epoch 18, cross_loss: 0.063, auc_train: 1.000, auc_valid:0.899, time: 0.14 sec\n",
      "epoch 19, cross_loss: 0.012, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 20, cross_loss: 0.003, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 21, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.902, time: 0.14 sec\n",
      "epoch 22, cross_loss: 0.031, auc_train: 1.000, auc_valid:0.902, time: 0.14 sec\n",
      "epoch 23, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.903, time: 0.14 sec\n",
      "epoch 24, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.909, time: 0.14 sec\n",
      "epoch 25, cross_loss: 0.005, auc_train: 1.000, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 26, cross_loss: 0.008, auc_train: 1.000, auc_valid:0.906, time: 0.14 sec\n",
      "epoch 27, cross_loss: 0.019, auc_train: 1.000, auc_valid:0.907, time: 0.14 sec\n",
      "epoch 28, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.904, time: 0.14 sec\n",
      "epoch 29, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.901, time: 0.14 sec\n",
      "epoch 30, cross_loss: 0.001, auc_train: 1.000, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 31, cross_loss: 0.003, auc_train: 1.000, auc_valid:0.892, time: 0.14 sec\n",
      "epoch 32, cross_loss: 0.018, auc_train: 1.000, auc_valid:0.892, time: 0.14 sec\n",
      "epoch 33, cross_loss: 0.000, auc_train: 1.000, auc_valid:0.895, time: 0.14 sec\n",
      "epoch 34, cross_loss: 0.002, auc_train: 1.000, auc_valid:0.894, time: 0.14 sec\n",
      "2000 2000\n",
      "neighbors 2000\n",
      "doing: LR-L1 2000 0\n",
      "doing: LR-L1 2000 1\n",
      "doing: LR-L1 2000 2\n",
      "doing: LR-L1 2000 3\n",
      "doing: LR-L1 2000 4\n",
      "doing: LR-L1 2000 5\n",
      "doing: LR-L1 2000 6\n",
      "doing: LR-L1 2000 7\n",
      "doing: LR-L1 2000 8\n",
      "doing: LR-L1 2000 9\n",
      "doing: DT 2000 0\n",
      "doing: DT 2000 1\n",
      "doing: DT 2000 2\n",
      "doing: DT 2000 3\n",
      "doing: DT 2000 4\n",
      "doing: DT 2000 5\n",
      "doing: DT 2000 6\n",
      "doing: DT 2000 7\n",
      "doing: DT 2000 8\n",
      "doing: DT 2000 9\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 2000 0\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 2000 1\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 2000 2\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 2000 3\n",
      "already done: CGN_3_layer_64_channel_emb_32_dropout 2000 4\n",
      "doing: CGN_3_layer_64_channel_emb_32_dropout 2000 5\n",
      "Doing drop-out\n"
     ]
    }
   ],
   "source": [
    "method_comparison(results, tcgatissue, m, gene=\"RPL5\", search_num_genes=[100,500,2000, 4000, 8000], trials=10, search_train_size=[200, 500], test_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(results, open(\"results.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = pickle.load(open(\"results.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results[\"df\"] = results[\"df\"].drop(results[\"df\"][results[\"df\"].num_genes == 56].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_name</th>\n",
       "      <th>model</th>\n",
       "      <th>train_size</th>\n",
       "      <th>num_genes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"61\" valign=\"top\">RPL5</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">CGN_3_layer_64_channel_emb_32_dropout</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">50</th>\n",
       "      <th>100</th>\n",
       "      <td>0.879169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.908477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.840535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.778975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.742106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.735848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">100</th>\n",
       "      <th>100</th>\n",
       "      <td>0.916422</td>\n",
       "      <td>0.013041</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.917243</td>\n",
       "      <td>0.015980</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.912943</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.898360</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.866555</td>\n",
       "      <td>0.012641</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.855745</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">200</th>\n",
       "      <th>100</th>\n",
       "      <td>0.935966</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.940450</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.937073</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.929117</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.920891</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.910035</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">300</th>\n",
       "      <th>100</th>\n",
       "      <td>0.947150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.945834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.943321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.943137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.931218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.934811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">500</th>\n",
       "      <th>100</th>\n",
       "      <td>0.936009</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.947665</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.945879</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1000</th>\n",
       "      <th>100</th>\n",
       "      <td>0.946984</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.959470</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.957809</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">MLP-dropout</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">50</th>\n",
       "      <th>500</th>\n",
       "      <td>0.840015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.813328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.780276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.734636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">100</th>\n",
       "      <th>100</th>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.921856</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.911874</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.891031</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.875491</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.861412</td>\n",
       "      <td>0.011076</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">200</th>\n",
       "      <th>100</th>\n",
       "      <td>0.931688</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.931286</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.929359</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.921708</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.913653</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.911140</td>\n",
       "      <td>0.015057</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">300</th>\n",
       "      <th>100</th>\n",
       "      <td>0.935083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.938064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.933859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.936616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.926049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.925105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">500</th>\n",
       "      <th>100</th>\n",
       "      <td>0.930644</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.937108</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.932695</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1000</th>\n",
       "      <th>100</th>\n",
       "      <td>0.942557</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.953444</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.950943</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.943458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.929050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          mean  \\\n",
       "gene_name model                                 train_size num_genes             \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 50         100        0.879169   \n",
       "                                                           200        0.908477   \n",
       "                                                           500        0.840535   \n",
       "                                                           1000       0.778975   \n",
       "                                                           1500       0.742106   \n",
       "                                                           2000       0.735848   \n",
       "                                                100        100        0.916422   \n",
       "                                                           200        0.917243   \n",
       "                                                           500        0.912943   \n",
       "                                                           1000       0.898360   \n",
       "                                                           1500       0.866555   \n",
       "                                                           2000       0.855745   \n",
       "                                                200        100        0.935966   \n",
       "                                                           200        0.940450   \n",
       "                                                           500        0.937073   \n",
       "                                                           1000       0.929117   \n",
       "                                                           1500       0.920891   \n",
       "                                                           2000       0.910035   \n",
       "                                                300        100        0.947150   \n",
       "                                                           200        0.945834   \n",
       "                                                           500        0.943321   \n",
       "                                                           1000       0.943137   \n",
       "                                                           1500       0.931218   \n",
       "                                                           2000       0.934811   \n",
       "                                                500        100        0.936009   \n",
       "                                                           500        0.947665   \n",
       "                                                           2000       0.945879   \n",
       "                                                1000       100        0.946984   \n",
       "                                                           500        0.959470   \n",
       "                                                           2000       0.957809   \n",
       "...                                                                        ...   \n",
       "          MLP-dropout                           50         500        0.840015   \n",
       "                                                           1000       0.813328   \n",
       "                                                           1500       0.780276   \n",
       "                                                           2000       0.734636   \n",
       "                                                100        100        0.908600   \n",
       "                                                           200        0.921856   \n",
       "                                                           500        0.911874   \n",
       "                                                           1000       0.891031   \n",
       "                                                           1500       0.875491   \n",
       "                                                           2000       0.861412   \n",
       "                                                200        100        0.931688   \n",
       "                                                           200        0.931286   \n",
       "                                                           500        0.929359   \n",
       "                                                           1000       0.921708   \n",
       "                                                           1500       0.913653   \n",
       "                                                           2000       0.911140   \n",
       "                                                300        100        0.935083   \n",
       "                                                           200        0.938064   \n",
       "                                                           500        0.933859   \n",
       "                                                           1000       0.936616   \n",
       "                                                           1500       0.926049   \n",
       "                                                           2000       0.925105   \n",
       "                                                500        100        0.930644   \n",
       "                                                           500        0.937108   \n",
       "                                                           2000       0.932695   \n",
       "                                                1000       100        0.942557   \n",
       "                                                           500        0.953444   \n",
       "                                                           2000       0.950943   \n",
       "                                                           4000       0.943458   \n",
       "                                                           8000       0.929050   \n",
       "\n",
       "                                                                           std  \\\n",
       "gene_name model                                 train_size num_genes             \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 50         100             NaN   \n",
       "                                                           200             NaN   \n",
       "                                                           500             NaN   \n",
       "                                                           1000            NaN   \n",
       "                                                           1500            NaN   \n",
       "                                                           2000            NaN   \n",
       "                                                100        100        0.013041   \n",
       "                                                           200        0.015980   \n",
       "                                                           500        0.014849   \n",
       "                                                           1000       0.007701   \n",
       "                                                           1500       0.012641   \n",
       "                                                           2000       0.023611   \n",
       "                                                200        100        0.010664   \n",
       "                                                           200        0.007943   \n",
       "                                                           500        0.013434   \n",
       "                                                           1000       0.012585   \n",
       "                                                           1500       0.012223   \n",
       "                                                           2000       0.020542   \n",
       "                                                300        100             NaN   \n",
       "                                                           200             NaN   \n",
       "                                                           500             NaN   \n",
       "                                                           1000            NaN   \n",
       "                                                           1500            NaN   \n",
       "                                                           2000            NaN   \n",
       "                                                500        100        0.002499   \n",
       "                                                           500        0.004307   \n",
       "                                                           2000       0.001785   \n",
       "                                                1000       100        0.003426   \n",
       "                                                           500        0.003350   \n",
       "                                                           2000       0.000928   \n",
       "...                                                                        ...   \n",
       "          MLP-dropout                           50         500             NaN   \n",
       "                                                           1000            NaN   \n",
       "                                                           1500            NaN   \n",
       "                                                           2000            NaN   \n",
       "                                                100        100        0.012822   \n",
       "                                                           200        0.011331   \n",
       "                                                           500        0.015511   \n",
       "                                                           1000       0.013103   \n",
       "                                                           1500       0.015960   \n",
       "                                                           2000       0.011076   \n",
       "                                                200        100        0.007613   \n",
       "                                                           200        0.008874   \n",
       "                                                           500        0.004242   \n",
       "                                                           1000       0.010704   \n",
       "                                                           1500       0.006206   \n",
       "                                                           2000       0.015057   \n",
       "                                                300        100             NaN   \n",
       "                                                           200             NaN   \n",
       "                                                           500             NaN   \n",
       "                                                           1000            NaN   \n",
       "                                                           1500            NaN   \n",
       "                                                           2000            NaN   \n",
       "                                                500        100        0.005218   \n",
       "                                                           500        0.006136   \n",
       "                                                           2000       0.006097   \n",
       "                                                1000       100        0.003989   \n",
       "                                                           500        0.004889   \n",
       "                                                           2000       0.005443   \n",
       "                                                           4000            NaN   \n",
       "                                                           8000            NaN   \n",
       "\n",
       "                                                                      count  \n",
       "gene_name model                                 train_size num_genes         \n",
       "RPL5      CGN_3_layer_64_channel_emb_32_dropout 50         100            1  \n",
       "                                                           200            1  \n",
       "                                                           500            1  \n",
       "                                                           1000           1  \n",
       "                                                           1500           1  \n",
       "                                                           2000           1  \n",
       "                                                100        100            5  \n",
       "                                                           200            5  \n",
       "                                                           500            5  \n",
       "                                                           1000           5  \n",
       "                                                           1500           5  \n",
       "                                                           2000           5  \n",
       "                                                200        100            5  \n",
       "                                                           200            5  \n",
       "                                                           500            5  \n",
       "                                                           1000           5  \n",
       "                                                           1500           5  \n",
       "                                                           2000           5  \n",
       "                                                300        100            1  \n",
       "                                                           200            1  \n",
       "                                                           500            1  \n",
       "                                                           1000           1  \n",
       "                                                           1500           1  \n",
       "                                                           2000           1  \n",
       "                                                500        100            3  \n",
       "                                                           500            3  \n",
       "                                                           2000           3  \n",
       "                                                1000       100            2  \n",
       "                                                           500            2  \n",
       "                                                           2000           2  \n",
       "...                                                                     ...  \n",
       "          MLP-dropout                           50         500            1  \n",
       "                                                           1000           1  \n",
       "                                                           1500           1  \n",
       "                                                           2000           1  \n",
       "                                                100        100            5  \n",
       "                                                           200            5  \n",
       "                                                           500            5  \n",
       "                                                           1000           5  \n",
       "                                                           1500           5  \n",
       "                                                           2000           5  \n",
       "                                                200        100            5  \n",
       "                                                           200            5  \n",
       "                                                           500            5  \n",
       "                                                           1000           5  \n",
       "                                                           1500           5  \n",
       "                                                           2000           5  \n",
       "                                                300        100            1  \n",
       "                                                           200            1  \n",
       "                                                           500            1  \n",
       "                                                           1000           1  \n",
       "                                                           1500           1  \n",
       "                                                           2000           1  \n",
       "                                                500        100            3  \n",
       "                                                           500            3  \n",
       "                                                           2000           3  \n",
       "                                                1000       100            2  \n",
       "                                                           500            2  \n",
       "                                                           2000           2  \n",
       "                                                           4000           1  \n",
       "                                                           8000           1  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = results[\"df\"].groupby(['gene_name', 'model','train_size','num_genes'])['auc'].agg(['mean','std', 'count'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results[\"df\"].groupby(['gene_name', 'model','num_genes'])['auc'].mean().groupby([\"model\"]).plot(legend=True, sharex=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">CGN_3_layer_64_channel_emb_32_dropout</th>\n",
       "      <th>50</th>\n",
       "      <td>0.908477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.917243</td>\n",
       "      <td>0.015980</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.940450</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.945834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MLP-dropout</th>\n",
       "      <th>50</th>\n",
       "      <td>0.905928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.921856</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.931286</td>\n",
       "      <td>0.008874</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.938064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       auc                \n",
       "                                                      mean       std count\n",
       "model                                 train_size                          \n",
       "CGN_3_layer_64_channel_emb_32_dropout 50          0.908477       NaN     1\n",
       "                                      100         0.917243  0.015980     5\n",
       "                                      200         0.940450  0.007943     5\n",
       "                                      300         0.945834       NaN     1\n",
       "MLP-dropout                           50          0.905928       NaN     1\n",
       "                                      100         0.921856  0.011331     5\n",
       "                                      200         0.931286  0.008874     5\n",
       "                                      300         0.938064       NaN     1"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"df\"][results[\"df\"].num_genes==200].groupby(['model','train_size']).agg(['mean','std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5J2Fc865lLyzcM45l9L/Aa5Sf44EueT8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf2559f250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for model in results[\"df\"][\"model\"].unique():\n",
    "    q = results[\"df\"][results[\"df\"].num_genes==100].groupby(['model','train_size'])['auc']\n",
    "    index = q.mean()[model].index\n",
    "    mean = q.mean()[model]\n",
    "    stderr = q.std()[model]/np.sqrt(q.count()[model])\n",
    "    plt.errorbar(index, mean,label=model, xerr=0, yerr=stderr)\n",
    "\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"train_size\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.xticks(index)\n",
    "formatter = matplotlib.ticker.ScalarFormatter()\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n05PFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf25ce0310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for model in results[\"df\"][\"model\"].unique():\n",
    "    q = results[\"df\"][results[\"df\"].train_size==200].groupby(['model','num_genes'])['auc']\n",
    "    index = q.mean()[model].index\n",
    "    mean = q.mean()[model]\n",
    "    stderr = q.std()[model]/np.sqrt(q.count()[model])\n",
    "    plt.errorbar(index, mean,label=model, xerr=0, yerr=stderr)\n",
    "\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(sorted(results[\"df\"][\"num_genes\"].unique()))\n",
    "formatter = matplotlib.ticker.ScalarFormatter()\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff76ebf4110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for model in results[\"df\"][\"model\"].unique():\n",
    "    index = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model].index\n",
    "    mean = results[\"df\"].groupby(['model','num_genes'])['auc'].mean()[model]\n",
    "    stderr = results[\"df\"].groupby(['model','num_genes'])['auc'].std()[model]\n",
    "    plt.errorbar(index, mean,label=model, xerr=0, yerr=stderr)\n",
    "\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(sorted(results[\"df\"][\"num_genes\"].unique()))\n",
    "formatter = matplotlib.ticker.ScalarFormatter()\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#results_df = pd.DataFrame(columns=['model', 'num_genes', 'gene_name', 'auc', 'std'])\n",
    "#results_df = results_df.append(data=pd.DataFrame(pd.DataFrame(data={'model':\"LR\", 'num_genes': 10.0, 'gene_name': \"RPL5\", 'auc':0.57, 'std': 0.01}, index=[0]))\n",
    "len([\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"])\n",
    "len([10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0])\n",
    "len([0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92])\n",
    "len([0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00])\n",
    "results_df = pd.DataFrame(data={'model':[\"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\", \"LR\", \"MLP\", \"Decision Tree\", \"CGN_3_layer_64_channel_emb_32_dropout\"],\n",
    "                   'num_genes': [10.0, 10.0, 10.0, 10.0, 510.0, 510.0, 510.0, 510.0, 1010.0, 1010.0, 1010.0, 1010.0, 1510.0, 1510.0, 1510.0, 1510.0, 2010.0, 2010.0, 2010.0, 2010.0, 2510.0, 2510.0, 2510.0, 2510.0, 3010.0, 3010.0, 3010.0, 3010.0],\n",
    "#                  'gene_name': [\"RPL5\", \"RPL5\"],\n",
    "                   'auc': [0.57, 0.56, 0.55, 0.64, 0.81, 0.83, .79, .94, .81, .80, .78, .94, .80, .74, .77, .93, .78, .79, .78, .92, .77, .77, .76, .92, .76, .71, .76, .92],\n",
    "                   'std': [0.01, 0.04, 0.03, 0.01, 0.02, 0.01, 0.03, 0.00, 0.01, 0.03, 0.02, 0.01, 0.03, 0.12, 0.02, 0.00, 0.03, 0.02, 0.03, 0.01, 0.02, 0.05, 0.04, 0.01, 0.02 ,0.11, 0.02, 0.00]}, index=range(0, 28))\n",
    "plt.figure()\n",
    "titles = []\n",
    "for model in [\n",
    "    {'key': 'LR', 'method': lr},\n",
    "    {'key': 'MLP', 'method': mlp},\n",
    "    {'key': 'Decision Tree', 'method': decision_tree},\n",
    "    {'key': 'CGN_3_layer_64_channel_emb_32_dropout', 'method': cgn_loop, 'num_channel': 64, 'num_layer': 3, 'add_emb': 32, 'use_gate': False, 'dropout': True, 'cuda': True},\n",
    "    ]:\n",
    "    temp_results = results_df.loc[results_df['model'] == model['key']].reset_index(drop=True)\n",
    "    lines.append(plt.errorbar(temp_results.index, temp_results['auc'], xerr=0, yerr=temp_results['std'])[0])\n",
    "    titles.append(model['key'])\n",
    "    plt.xticks(list(temp_results.index), temp_results['num_genes'], rotation=70)\n",
    "width = 0.2\n",
    "plt.title(\"Inferring the value of RPL5 with varying numbers of genes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"# genes\")\n",
    "plt.legend(lines, titles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_genes'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of nodes\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a gene from a growing number of Nodes\n",
    "lr_results = pd.DataFrame([])\n",
    "mlp_results = pd.DataFrame([])\n",
    "cgn_results = pd.DataFrame([])\n",
    "gene = \"RPL5\"\n",
    "max_samples = 200\n",
    "reload(data)\n",
    "reload(models)\n",
    "tcgatissue = data.gene_datasets.TCGATissue(data_dir='./genomics/TCGA/', data_file='TCGA_tissue_ppi.hdf5')\n",
    "\n",
    "for num_samples in range(10, max_samples, 20):\n",
    "    lr_row = infer_gene(lr, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    lr_results = lr_results.append(lr_row).reset_index(drop=True)\n",
    "    lr_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    cgn_row = infer_gene(cgn, tcgatissue, \"RPL5\", train_size=num_samples, test_size=200, trials=3, penalty=True)\n",
    "    cgn_results = cgn_results.append(cgn_row).reset_index(drop=True)\n",
    "    cgn_results.loc[lr_results.index[-1], 'num_samples'] = num_samples\n",
    "    print num_genes\n",
    "    print cgn_results\n",
    "    print lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results of adding Nodes\n",
    "plt.figure()\n",
    "\n",
    "#full_results.loc[full_results['samples'] == 100]\n",
    "\n",
    "line1 = plt.errorbar(lr_results.index, lr_results['auc'], xerr=0, yerr=lr_results['std'])\n",
    "line2 = plt.errorbar(cgn_results.index, cgn_results['auc'], xerr=0, yerr=cgn_results['std'])\n",
    "\n",
    "width = 0.2\n",
    "plt.xticks(list(lr_results.iloc[::5, :].index), lr_results.iloc[::5, :]['num_samples'], rotation=70)\n",
    "plt.title(\"Gene Inference with varying numbers of samples\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"number of samples\")\n",
    "plt.legend((line1[0], line2[0]), ('LR', \"CGN\"), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
